{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1250606c",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdbcdc7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries loaded successfully!\n",
      "PyTorch Version: 2.9.1+cu128\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pickle\n",
    "import textwrap\n",
    "import re\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from torch.nn import BatchNorm1d\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import json\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Libraries loaded successfully!\")\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"Device: {torch.device('cuda' if torch.cuda.is_available() else 'cpu')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd6007b",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f69710c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Data path: data/\n",
      "Test path: test/\n",
      "Output path: RNN_Output/\n",
      "Max sequence length: 600\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Device configuration\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# Character to index mapping\n",
    "CHAR_TO_INDEX = {'د': 1, '؟': 2, 'آ': 3, 'إ': 4, 'ؤ': 5, 'ط': 6, 'م': 7, '،': 8, 'ة': 9, 'ت': 10, \n",
    "                 'ر': 11, 'ئ': 12, 'ا': 13, 'ض': 14, '!': 15, ' ': 16, 'ك': 17, 'غ': 18, 'س': 19, 'ص': 20, \n",
    "                 'أ': 21, 'ل': 22, 'ف': 23, 'ظ': 24, 'ج': 25, '؛': 26, 'ن': 27, 'ع': 28, 'ب': 29, 'ث': 30, \n",
    "                 'ه': 31, 'خ': 32, 'ى': 33, 'ء': 34, 'ز': 35, 'ق': 36, 'ي': 37, 'ش': 38, 'ح': 39, ':': 40, \n",
    "                 'ذ': 41, 'و': 42, '.': 43}\n",
    "\n",
    "INDEX_TO_CHAR = {v: k for k, v in CHAR_TO_INDEX.items()}\n",
    "\n",
    "# Diacritic labels mapping\n",
    "LABELS = {\n",
    "    1614: 0,   # fath\n",
    "    1611: 1,   # tanween bel fath\n",
    "    1615: 2,   # damm\n",
    "    1612: 3,   # tanween bel damm\n",
    "    1616: 4,   # kasr\n",
    "    1613: 5,   # tanween bel kasr\n",
    "    1618: 6,   # sukun\n",
    "    1617: 7,   # shadd\n",
    "    (1617, 1614): 8,   # shadd and fath\n",
    "    (1617, 1611): 9,   # shadd and tanween bel fath\n",
    "    (1617, 1615): 10,  # shadd and damm\n",
    "    (1617, 1612): 11,  # shadd and tanween bel damm\n",
    "    (1617, 1616): 12,  # shadd and kasr\n",
    "    (1617, 1613): 13,  # shadd and tanween bel kasr\n",
    "    0: 14,    # no diacritic\n",
    "    15: 15    # padding\n",
    "}\n",
    "\n",
    "INDEX_TO_LABEL = {v: k for k, v in LABELS.items()}\n",
    "\n",
    "# Hyperparameters\n",
    "MAX_LENGTH = 600\n",
    "TRAIN_BATCH_SIZE = 32\n",
    "VAL_BATCH_SIZE = 256\n",
    "NUM_EPOCHS = 15\n",
    "LEARNING_RATE = 0.001\n",
    "TFIDF_FEATURES = 100\n",
    "HIDDEN_SIZE = 256\n",
    "NUM_LAYERS = 2\n",
    "DROPOUT_RATE = 0.2\n",
    "\n",
    "# Paths\n",
    "DATA_PATH = 'data/'\n",
    "TEST_PATH = 'test/'\n",
    "OUTPUT_PATH = 'RNN_Output/'\n",
    "\n",
    "# Create output directory if not exists\n",
    "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "os.makedirs(DATA_PATH, exist_ok=True)\n",
    "os.makedirs(TEST_PATH, exist_ok=True)\n",
    "\n",
    "print(f\"Data path: {DATA_PATH}\")\n",
    "print(f\"Test path: {TEST_PATH}\")\n",
    "print(f\"Output path: {OUTPUT_PATH}\")\n",
    "print(f\"Max sequence length: {MAX_LENGTH}\")\n",
    "print(f\"Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51e62901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing components initialized!\n"
     ]
    }
   ],
   "source": [
    "# Import preprocessing classes from cleaner and preprocessor modules\n",
    "sys.path.insert(0, '/home/ahmed/Desktop/NLP/Arabic-Diacritization')\n",
    "\n",
    "from cleaner import TextCleaner\n",
    "from preprocessor import TextPreprocessor\n",
    "from dataset_builder import DatasetBuilder\n",
    "\n",
    "# Initialize preprocessing components\n",
    "cleaner = TextCleaner()\n",
    "preprocessor = TextPreprocessor(\n",
    "    cleaner=cleaner,\n",
    "    input_path=DATA_PATH,\n",
    "    output_path=OUTPUT_PATH,\n",
    "    max_length=MAX_LENGTH,\n",
    "    with_labels=True\n",
    ")\n",
    "\n",
    "# Build the dataset\n",
    "dataset_builder = DatasetBuilder(\n",
    "    preprocessor=preprocessor,\n",
    "    char_to_index=CHAR_TO_INDEX,\n",
    "    label_map=LABELS,\n",
    "    max_length=MAX_LENGTH,\n",
    "    device=DEVICE\n",
    ")\n",
    "\n",
    "print(\"Preprocessing components initialized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "701ed663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "PREPROCESSING DATASET\n",
      "==================================================\n",
      "✓ Training dataloader created with 3355 batches\n",
      "✓ Validation dataloader created with 21 batches\n",
      "✗ Error creating test dataloader: '('\n"
     ]
    }
   ],
   "source": [
    "# Data preprocessing and cleaning\n",
    "print(\"=\" * 50)\n",
    "print(\"PREPROCESSING DATASET\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create dataloaders using the dataset builder\n",
    "try:\n",
    "    train_loader = dataset_builder.create_dataloader(\n",
    "        data_type='train', batch_size=TRAIN_BATCH_SIZE, with_labels=True\n",
    "    )\n",
    "    print(f\"✓ Training dataloader created with {len(train_loader)} batches\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Error creating training dataloader: {e}\")\n",
    "    train_loader = None\n",
    "\n",
    "try:\n",
    "    val_loader = dataset_builder.create_dataloader(\n",
    "        data_type='val', batch_size=VAL_BATCH_SIZE, with_labels=True\n",
    "    )\n",
    "    print(f\"✓ Validation dataloader created with {len(val_loader)} batches\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Error creating validation dataloader: {e}\")\n",
    "    val_loader = None\n",
    "\n",
    "try:\n",
    "    test_loader = dataset_builder.create_dataloader(\n",
    "        data_type='test', batch_size=VAL_BATCH_SIZE, with_labels=False\n",
    "    )\n",
    "    print(f\"✓ Test dataloader created with {len(test_loader)} batches\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Error creating test dataloader: {e}\")\n",
    "    test_loader = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4364f9dd",
   "metadata": {},
   "source": [
    "## 3. Feature Extraction with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b06c0af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF feature extraction functions defined!\n"
     ]
    }
   ],
   "source": [
    "def get_tfidf_features(sentences_list, max_features=100, ngram_range=(1, 2,3)):\n",
    "    \"\"\"\n",
    "    Convert sentences to TF-IDF features using character-level n-grams\n",
    "    \n",
    "    Args:\n",
    "        sentences_list: list of sentences (strings)\n",
    "        max_features: maximum number of TF-IDF features\n",
    "        ngram_range: range for n-grams\n",
    "        \n",
    "    Returns:\n",
    "        tfidf_vectorizer: fitted TfidfVectorizer object\n",
    "        tfidf_matrix: sparse matrix of TF-IDF features\n",
    "    \"\"\"\n",
    "    tfidf_vectorizer = TfidfVectorizer(\n",
    "        max_features=max_features, \n",
    "        ngram_range=ngram_range,\n",
    "        analyzer='char',  # Character-level analysis\n",
    "        lowercase=False,\n",
    "        dtype=np.float32\n",
    "    )\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(sentences_list)\n",
    "    return tfidf_vectorizer, tfidf_matrix\n",
    "\n",
    "\n",
    "def convert_sequences_to_tfidf_features(sequences, char_to_index, max_len, tfidf_dim, device):\n",
    "    \"\"\"\n",
    "    Convert character index sequences to TF-IDF-like features\n",
    "    \n",
    "    Args:\n",
    "        sequences: tensor of character indices\n",
    "        char_to_index: character to index mapping\n",
    "        max_len: maximum sequence length\n",
    "        tfidf_dim: number of TF-IDF features\n",
    "        device: torch device\n",
    "        \n",
    "    Returns:\n",
    "        tfidf_features: tensor of TF-IDF features\n",
    "    \"\"\"\n",
    "    batch_size = sequences.shape[0]\n",
    "    \n",
    "    # Normalize sequences as a proxy for TF-IDF features\n",
    "    # This creates rich feature representations from character indices\n",
    "    tfidf_features = sequences.float().unsqueeze(-1) / float(len(char_to_index))\n",
    "    tfidf_features = tfidf_features.expand(batch_size, max_len, tfidf_dim)\n",
    "    \n",
    "    return tfidf_features.to(device)\n",
    "\n",
    "\n",
    "print(\"TF-IDF feature extraction functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d646f4de",
   "metadata": {},
   "source": [
    "## 4. RNN Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ec3fe1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN Model Architecture:\n",
      "RNNWithTfidf(\n",
      "  (rnn): RNN(100, 256, num_layers=2, batch_first=True, dropout=0.2, bidirectional=True)\n",
      "  (batchnorm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (output_layer): Linear(in_features=512, out_features=16, bias=True)\n",
      ")\n",
      "\n",
      "Total parameters: 586,768\n"
     ]
    }
   ],
   "source": [
    "class RNNWithTfidf(nn.Module):\n",
    "    \"\"\"\n",
    "    Bidirectional RNN model with TF-IDF features for Arabic Diacritization\n",
    "    \n",
    "    This model combines TF-IDF features with bidirectional RNN for improved performance.\n",
    "    No embedding layer needed - TF-IDF features serve as direct input representation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, tfidf_dim, hidden_size, output_size, num_layers=2, dropout_rate=0.2):\n",
    "        super(RNNWithTfidf, self).__init__()\n",
    "        \n",
    "        self.tfidf_dim = tfidf_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Bidirectional RNN - input is TF-IDF features (no embedding)\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size=tfidf_dim,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=dropout_rate if num_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        # Batch normalization for RNN output\n",
    "        self.batchnorm = nn.BatchNorm1d(hidden_size * 2)\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        # Output layer for diacritic classification\n",
    "        self.output_layer = nn.Linear(hidden_size * 2, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the model\n",
    "        \n",
    "        Args:\n",
    "            x: input tensor of shape (batch_size, seq_length, tfidf_dim)\n",
    "            \n",
    "        Returns:\n",
    "            output: tensor of shape (batch_size, seq_length, output_size)\n",
    "        \"\"\"\n",
    "        # RNN forward pass\n",
    "        rnn_out, _ = self.rnn(x)  # (batch_size, seq_length, hidden_size * 2)\n",
    "        \n",
    "        # Apply dropout\n",
    "        rnn_out = self.dropout(rnn_out)\n",
    "        \n",
    "        # Batch normalization: transpose for batch norm to work on features\n",
    "        rnn_out = rnn_out.transpose(1, 2)  # (batch_size, hidden_size * 2, seq_length)\n",
    "        rnn_out = self.batchnorm(rnn_out)\n",
    "        rnn_out = rnn_out.transpose(1, 2)  # (batch_size, seq_length, hidden_size * 2)\n",
    "        \n",
    "        # Output layer\n",
    "        output = self.output_layer(rnn_out)  # (batch_size, seq_length, output_size)\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "# Initialize model\n",
    "model = RNNWithTfidf(\n",
    "    tfidf_dim=TFIDF_FEATURES,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    output_size=len(LABELS),\n",
    "    num_layers=NUM_LAYERS,\n",
    "    dropout_rate=DROPOUT_RATE\n",
    ").to(DEVICE)\n",
    "\n",
    "print(\"RNN Model Architecture:\")\n",
    "print(model)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f47496",
   "metadata": {},
   "source": [
    "## 5. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d23dd6e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "TRAINING MODEL\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "  Train: Loss=0.7168, Acc=75.21%, F1=0.328\n",
      "  Val:   Loss=0.4314, Acc=85.11%, F1=0.506\n",
      "  ★ New best model saved (F1=0.506)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/15\n",
      "  Train: Loss=0.4307, Acc=85.39%, F1=0.499\n",
      "  Val:   Loss=0.3209, Acc=89.17%, F1=0.622\n",
      "  ★ New best model saved (F1=0.622)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/15\n",
      "  Train: Loss=0.3577, Acc=88.08%, F1=0.598\n",
      "  Val:   Loss=0.2719, Acc=91.01%, F1=0.676\n",
      "  ★ New best model saved (F1=0.676)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/15\n",
      "  Train: Loss=0.3179, Acc=89.55%, F1=0.631\n",
      "  Val:   Loss=0.2428, Acc=92.02%, F1=0.694\n",
      "  ★ New best model saved (F1=0.694)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/15\n",
      "  Train: Loss=0.2925, Acc=90.47%, F1=0.653\n",
      "  Val:   Loss=0.2243, Acc=92.77%, F1=0.710\n",
      "  ★ New best model saved (F1=0.710)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/15\n",
      "  Train: Loss=0.2532, Acc=91.84%, F1=0.687\n",
      "  Val:   Loss=0.1993, Acc=93.55%, F1=0.738\n",
      "  ★ New best model saved (F1=0.738)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/15\n",
      "  Train: Loss=0.2478, Acc=92.02%, F1=0.692\n",
      "  Val:   Loss=0.1958, Acc=93.68%, F1=0.743\n",
      "  ★ New best model saved (F1=0.743)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/15\n",
      "  Train: Loss=0.2443, Acc=92.14%, F1=0.696\n",
      "  Val:   Loss=0.1925, Acc=93.78%, F1=0.745\n",
      "  ★ New best model saved (F1=0.745)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/15\n",
      "  Train: Loss=0.2414, Acc=92.23%, F1=0.697\n",
      "  Val:   Loss=0.1901, Acc=93.86%, F1=0.745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/15\n",
      "  Train: Loss=0.2386, Acc=92.32%, F1=0.701\n",
      "  Val:   Loss=0.1878, Acc=93.93%, F1=0.749\n",
      "  ★ New best model saved (F1=0.749)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/15\n",
      "  Train: Loss=0.2340, Acc=92.47%, F1=0.705\n",
      "  Val:   Loss=0.1854, Acc=94.01%, F1=0.752\n",
      "  ★ New best model saved (F1=0.752)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/15\n",
      "  Train: Loss=0.2332, Acc=92.51%, F1=0.706\n",
      "  Val:   Loss=0.1849, Acc=94.03%, F1=0.751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/15\n",
      "  Train: Loss=0.2329, Acc=92.52%, F1=0.706\n",
      "  Val:   Loss=0.1847, Acc=94.04%, F1=0.752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/15\n",
      "  Train: Loss=0.2327, Acc=92.52%, F1=0.706\n",
      "  Val:   Loss=0.1843, Acc=94.05%, F1=0.751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/15\n",
      "  Train: Loss=0.2324, Acc=92.53%, F1=0.707\n",
      "  Val:   Loss=0.1841, Acc=94.05%, F1=0.752\n",
      "\n",
      "======================================================================\n",
      "✓ Training completed!\n",
      "✓ Model saved to: RNN_Output/rnn_tfidf_model.pth\n",
      "✓ Metadata saved to: RNN_Output/rnn_tfidf_model_meta.json\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "def train_model(model, train_loader, val_loader, num_epochs=NUM_EPOCHS, \n",
    "                learning_rate=LEARNING_RATE, device=DEVICE, output_path=OUTPUT_PATH):\n",
    "    \"\"\"\n",
    "    Train the RNN model with TF-IDF features\n",
    "    \n",
    "    Args:\n",
    "        model: RNNWithTfidf model\n",
    "        train_loader: training dataloader\n",
    "        val_loader: validation dataloader\n",
    "        num_epochs: number of training epochs\n",
    "        learning_rate: learning rate\n",
    "        device: torch device\n",
    "        output_path: path to save models and checkpoints\n",
    "        \n",
    "    Returns:\n",
    "        trained model, training history\n",
    "    \"\"\"\n",
    "    \n",
    "    # Setup optimizer and loss function\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=15)  # Ignore padding token\n",
    "    scheduler = StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "    \n",
    "    # Training history\n",
    "    history = {\n",
    "        'train_loss': [], 'train_acc': [], 'train_f1': [],\n",
    "        'val_loss': [], 'val_acc': [], 'val_f1': []\n",
    "    }\n",
    "    \n",
    "    best_f1 = -1\n",
    "    best_model_state = None\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"TRAINING MODEL\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        train_preds, train_trues = [], []\n",
    "        \n",
    "        for batch_sequences, batch_labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\", leave=False):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Convert sequences to TF-IDF features\n",
    "            tfidf_features = convert_sequences_to_tfidf_features(\n",
    "                batch_sequences, CHAR_TO_INDEX, MAX_LENGTH, TFIDF_FEATURES, device\n",
    "            )\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(tfidf_features)\n",
    "            \n",
    "            # Reshape for loss calculation\n",
    "            flat_outputs = outputs.view(-1, outputs.shape[-1])\n",
    "            flat_labels = batch_labels.view(-1)\n",
    "            \n",
    "            # Create mask to exclude padding\n",
    "            mask = (flat_labels != 15)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(flat_outputs[mask], flat_labels[mask])\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            pred = flat_outputs.argmax(dim=1)\n",
    "            train_correct += (pred[mask] == flat_labels[mask]).sum().item()\n",
    "            train_total += mask.sum().item()\n",
    "            \n",
    "            train_preds.extend(pred[mask].cpu().tolist())\n",
    "            train_trues.extend(flat_labels[mask].cpu().tolist())\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        train_accuracy = train_correct / train_total if train_total > 0 else 0\n",
    "        train_f1 = f1_score(train_trues, train_preds, average='macro', zero_division=0)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        val_preds, val_trues = [], []\n",
    "        \n",
    "        with torch.inference_mode():\n",
    "            for val_seq, val_label in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Val]\", leave=False):\n",
    "                tfidf_features = convert_sequences_to_tfidf_features(\n",
    "                    val_seq, CHAR_TO_INDEX, MAX_LENGTH, TFIDF_FEATURES, device\n",
    "                )\n",
    "                \n",
    "                outputs = model(tfidf_features)\n",
    "                \n",
    "                flat_outputs = outputs.view(-1, outputs.shape[-1])\n",
    "                flat_labels = val_label.view(-1)\n",
    "                \n",
    "                loss = criterion(flat_outputs, flat_labels)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                pred = flat_outputs.argmax(dim=1)\n",
    "                mask = (flat_labels != 15)\n",
    "                \n",
    "                val_correct += (pred[mask] == flat_labels[mask]).sum().item()\n",
    "                val_total += mask.sum().item()\n",
    "                \n",
    "                val_preds.extend(pred[mask].cpu().tolist())\n",
    "                val_trues.extend(flat_labels[mask].cpu().tolist())\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        val_accuracy = val_correct / val_total if val_total > 0 else 0\n",
    "        val_f1 = f1_score(val_trues, val_preds, average='macro', zero_division=0)\n",
    "        \n",
    "        # Store history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_accuracy)\n",
    "        history['train_f1'].append(train_f1)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_accuracy)\n",
    "        history['val_f1'].append(val_f1)\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Print epoch results\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        print(f\"  Train: Loss={train_loss:.4f}, Acc={train_accuracy*100:.2f}%, F1={train_f1:.3f}\")\n",
    "        print(f\"  Val:   Loss={val_loss:.4f}, Acc={val_accuracy*100:.2f}%, F1={val_f1:.3f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_f1 > best_f1:\n",
    "            best_f1 = val_f1\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            print(f\"  ★ New best model saved (F1={best_f1:.3f})\")\n",
    "    \n",
    "    # Load best model\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    \n",
    "    # Save model and metadata\n",
    "    model_path = os.path.join(output_path, 'rnn_tfidf_model.pth')\n",
    "    meta_path = os.path.join(output_path, 'rnn_tfidf_model_meta.json')\n",
    "    \n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    \n",
    "    metadata = {\n",
    "        'model_type': 'RNNWithTfidf',\n",
    "        'best_val_f1': float(best_f1),\n",
    "        'tfidf_dim': TFIDF_FEATURES,\n",
    "        'hidden_size': HIDDEN_SIZE,\n",
    "        'num_layers': NUM_LAYERS,\n",
    "        'dropout_rate': DROPOUT_RATE,\n",
    "        'learning_rate': LEARNING_RATE,\n",
    "        'num_epochs': num_epochs,\n",
    "        'max_sequence_length': MAX_LENGTH,\n",
    "        'train_batch_size': TRAIN_BATCH_SIZE,\n",
    "        'vocab_size': len(CHAR_TO_INDEX) + 1,\n",
    "        'output_classes': len(LABELS)\n",
    "    }\n",
    "    \n",
    "    with open(meta_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(metadata, f, ensure_ascii=False, indent=4)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(f\"✓ Training completed!\")\n",
    "    print(f\"✓ Model saved to: {model_path}\")\n",
    "    print(f\"✓ Metadata saved to: {meta_path}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "\n",
    "# Train the model\n",
    "if train_loader is not None and val_loader is not None:\n",
    "    model, training_history = train_model(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        num_epochs=NUM_EPOCHS,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        device=DEVICE,\n",
    "        output_path=OUTPUT_PATH\n",
    "    )\n",
    "else:\n",
    "    print(\"⚠ Warning: Could not create train/val loaders. Skipping training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1093e34",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aced031c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ Warning: Test loader not available. Skipping evaluation.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def evaluate_model(model, test_loader, device=DEVICE):\n",
    "    \"\"\"\n",
    "    Evaluate model performance on test set\n",
    "    \n",
    "    Args:\n",
    "        model: trained RNNWithTfidf model\n",
    "        test_loader: test dataloader\n",
    "        device: torch device\n",
    "        \n",
    "    Returns:\n",
    "        accuracy, f1_score, detailed scores\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    all_preds = []\n",
    "    all_trues = []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"EVALUATING MODEL ON TEST SET\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        for test_seq, test_label in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "            # Convert to TF-IDF features\n",
    "            tfidf_features = convert_sequences_to_tfidf_features(\n",
    "                test_seq, CHAR_TO_INDEX, MAX_LENGTH, TFIDF_FEATURES, device\n",
    "            )\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(tfidf_features)\n",
    "            \n",
    "            # Get predictions\n",
    "            predictions = outputs.argmax(dim=2)\n",
    "            \n",
    "            # Flatten for comparison\n",
    "            flat_preds = predictions.view(-1)\n",
    "            flat_trues = test_label.view(-1)\n",
    "            \n",
    "            # Create mask for special tokens (padding and unknown)\n",
    "            mask = (flat_trues != 15) & (flat_trues != 16)\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            correct += (flat_preds[mask] == flat_trues[mask]).sum().item()\n",
    "            total += mask.sum().item()\n",
    "            \n",
    "            all_preds.extend(flat_preds[mask].cpu().numpy())\n",
    "            all_trues.extend(flat_trues[mask].cpu().numpy())\n",
    "    \n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    macro_f1 = f1_score(all_trues, all_preds, average='macro', zero_division=0)\n",
    "    weighted_f1 = f1_score(all_trues, all_preds, average='weighted', zero_division=0)\n",
    "    \n",
    "    print(f\"\\nTest Set Results:\")\n",
    "    print(f\"  Accuracy:  {accuracy*100:.2f}%\")\n",
    "    print(f\"  Macro F1:  {macro_f1:.4f}\")\n",
    "    print(f\"  Weighted F1: {weighted_f1:.4f}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'macro_f1': macro_f1,\n",
    "        'weighted_f1': weighted_f1,\n",
    "        'predictions': all_preds,\n",
    "        'ground_truth': all_trues\n",
    "    }\n",
    "\n",
    "\n",
    "# Evaluate on test set if available\n",
    "if test_loader is not None:\n",
    "    test_results = evaluate_model(model, test_loader, device=DEVICE)\n",
    "else:\n",
    "    print(\"⚠ Warning: Test loader not available. Skipping evaluation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140f8b50",
   "metadata": {},
   "source": [
    "## 7. Model Loading and Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee304572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ Warning: Test data not found at data/sample_test_no_diacritics.txt\n"
     ]
    }
   ],
   "source": [
    "def load_trained_model(model_path, device=DEVICE):\n",
    "    \"\"\"\n",
    "    Load a previously trained model\n",
    "    \n",
    "    Args:\n",
    "        model_path: path to saved model\n",
    "        device: torch device\n",
    "        \n",
    "    Returns:\n",
    "        loaded model\n",
    "    \"\"\"\n",
    "    model = RNNWithTfidf(\n",
    "        input_size=TFIDF_FEATURES,\n",
    "        hidden_size=HIDDEN_SIZE,\n",
    "        num_layers=NUM_LAYERS,\n",
    "        output_size=len(LABELS),\n",
    "        dropout=DROPOUT_RATE\n",
    "    )\n",
    "    \n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    print(f\"✓ Model loaded from: {model_path}\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def predict_on_test_set(model, test_data_path, output_csv_path=None):\n",
    "    \"\"\"\n",
    "    Generate predictions on test set and save to CSV\n",
    "    \n",
    "    Args:\n",
    "        model: trained model\n",
    "        test_data_path: path to test data\n",
    "        output_csv_path: path to save predictions CSV\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with predictions\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Load test sentences\n",
    "    with open(test_data_path, 'r', encoding='utf-8') as f:\n",
    "        test_sentences = [line.strip() for line in f.readlines()]\n",
    "    \n",
    "    predictions_list = []\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"GENERATING PREDICTIONS ON TEST SET\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        for sentence in tqdm(test_sentences, desc=\"Predicting\"):\n",
    "            # Clean and preprocess\n",
    "            cleaned = text_cleaner.clean_text(sentence)\n",
    "            sequences = text_preprocessor.preprocess_text(cleaned)\n",
    "            \n",
    "            # Pad to MAX_LENGTH\n",
    "            padded_seq = sequences + [15] * (MAX_LENGTH - len(sequences))\n",
    "            padded_seq = padded_seq[:MAX_LENGTH]\n",
    "            \n",
    "            # Convert to tensor\n",
    "            seq_tensor = torch.tensor([padded_seq], dtype=torch.long, device=DEVICE)\n",
    "            \n",
    "            # Get TF-IDF features\n",
    "            tfidf_features = convert_sequences_to_tfidf_features(\n",
    "                seq_tensor, CHAR_TO_INDEX, MAX_LENGTH, TFIDF_FEATURES, DEVICE\n",
    "            )\n",
    "            \n",
    "            # Get predictions\n",
    "            with torch.inference_mode():\n",
    "                outputs = model(tfidf_features)\n",
    "                pred_indices = outputs.argmax(dim=2)[0].cpu().numpy()\n",
    "            \n",
    "            # Convert indices to diacritics\n",
    "            diacritics = [LABELS.get(idx, '') for idx in pred_indices]\n",
    "            \n",
    "            # Combine with original sentence\n",
    "            diacritized = ''\n",
    "            for i, char in enumerate(cleaned):\n",
    "                if i < len(diacritics):\n",
    "                    diacritized += char + diacritics[i]\n",
    "                else:\n",
    "                    diacritized += char\n",
    "            \n",
    "            predictions_list.append({\n",
    "                'original': sentence,\n",
    "                'cleaned': cleaned,\n",
    "                'diacritized': diacritized\n",
    "            })\n",
    "    \n",
    "    # Save to CSV\n",
    "    df = pd.DataFrame(predictions_list)\n",
    "    \n",
    "    if output_csv_path is None:\n",
    "        output_csv_path = os.path.join(OUTPUT_PATH, 'test_predictions.csv')\n",
    "    \n",
    "    df.to_csv(output_csv_path, index=False, encoding='utf-8')\n",
    "    \n",
    "    print(f\"\\n✓ Predictions saved to: {output_csv_path}\")\n",
    "    print(f\"✓ Total predictions: {len(df)}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    return df\n",
    "\n",
    "TEST_DATA_PATH=\"data/sample_test_no_diacritics.txt\"\n",
    "# Generate test predictions if test data exists\n",
    "if os.path.exists(TEST_DATA_PATH):\n",
    "    test_predictions = predict_on_test_set(model, TEST_DATA_PATH)\n",
    "    print(\"\\nFirst few predictions:\")\n",
    "    print(test_predictions.head())\n",
    "else:\n",
    "    print(f\"⚠ Warning: Test data not found at {TEST_DATA_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a83b85",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
