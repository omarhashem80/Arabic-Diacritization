{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdbcdc7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pickle\n",
    "import textwrap\n",
    "import re\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from torch.nn import BatchNorm1d\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import json\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"Device: {torch.device('cuda' if torch.cuda.is_available() else 'cpu')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f69710c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "\n",
    "CHAR_TO_INDEX = {'د': 1, '؟': 2, 'آ': 3, 'إ': 4, 'ؤ': 5, 'ط': 6, 'م': 7, '،': 8, 'ة': 9, 'ت': 10, \n",
    "                 'ر': 11, 'ئ': 12, 'ا': 13, 'ض': 14, '!': 15, ' ': 16, 'ك': 17, 'غ': 18, 'س': 19, 'ص': 20, \n",
    "                 'أ': 21, 'ل': 22, 'ف': 23, 'ظ': 24, 'ج': 25, '؛': 26, 'ن': 27, 'ع': 28, 'ب': 29, 'ث': 30, \n",
    "                 'ه': 31, 'خ': 32, 'ى': 33, 'ء': 34, 'ز': 35, 'ق': 36, 'ي': 37, 'ش': 38, 'ح': 39, ':': 40, \n",
    "                 'ذ': 41, 'و': 42, '.': 43}\n",
    "\n",
    "INDEX_TO_CHAR = {v: k for k, v in CHAR_TO_INDEX.items()}\n",
    "\n",
    "\n",
    "LABELS = {\n",
    "    1614: 0,   \n",
    "    1611: 1,   \n",
    "    1615: 2,   \n",
    "    1612: 3,   \n",
    "    1616: 4,   \n",
    "    1613: 5,   \n",
    "    1618: 6,   \n",
    "    1617: 7,   \n",
    "    (1617, 1614): 8,   \n",
    "    (1617, 1611): 9,   \n",
    "    (1617, 1615): 10,  \n",
    "    (1617, 1612): 11,  \n",
    "    (1617, 1616): 12,  \n",
    "    (1617, 1613): 13,  \n",
    "    0: 14,    \n",
    "    15: 15    \n",
    "}\n",
    "\n",
    "INDEX_TO_LABEL = {v: k for k, v in LABELS.items()}\n",
    "\n",
    "\n",
    "MAX_LENGTH = 600\n",
    "TRAIN_BATCH_SIZE = 32\n",
    "VAL_BATCH_SIZE = 256\n",
    "NUM_EPOCHS = 15\n",
    "LEARNING_RATE = 0.001\n",
    "TFIDF_FEATURES = 100\n",
    "HIDDEN_SIZE = 256\n",
    "NUM_LAYERS = 2\n",
    "DROPOUT_RATE = 0.2\n",
    "\n",
    "\n",
    "DATA_PATH = 'data/'\n",
    "TEST_PATH = 'test/'\n",
    "OUTPUT_PATH = 'RNN_Output/'\n",
    "\n",
    "\n",
    "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "os.makedirs(DATA_PATH, exist_ok=True)\n",
    "os.makedirs(TEST_PATH, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51e62901",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sys.path.insert(0, './Arabic-Diacritization')\n",
    "\n",
    "from cleaner import TextCleaner\n",
    "from preprocessor import TextPreprocessor\n",
    "from dataset_builder import DatasetBuilder\n",
    "\n",
    "\n",
    "cleaner = TextCleaner()\n",
    "preprocessor = TextPreprocessor(\n",
    "    cleaner=cleaner,\n",
    "    input_path=DATA_PATH,\n",
    "    output_path=OUTPUT_PATH,\n",
    "    max_length=MAX_LENGTH,\n",
    "    with_labels=True\n",
    ")\n",
    "\n",
    "\n",
    "dataset_builder = DatasetBuilder(\n",
    "    preprocessor=preprocessor,\n",
    "    char_to_index=CHAR_TO_INDEX,\n",
    "    label_map=LABELS,\n",
    "    max_length=MAX_LENGTH,\n",
    "    device=DEVICE\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701ed663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "PREPROCESSING DATASET\n",
      "==================================================\n",
      "✓ Training dataloader created with 3355 batches\n",
      "✓ Validation dataloader created with 21 batches\n",
      "✗ Error creating test dataloader: '('\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    train_loader = dataset_builder.create_dataloader(\n",
    "        data_type='train', batch_size=TRAIN_BATCH_SIZE, with_labels=True\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Error creating training dataloader: {e}\")\n",
    "    train_loader = None\n",
    "\n",
    "try:\n",
    "    val_loader = dataset_builder.create_dataloader(\n",
    "        data_type='val', batch_size=VAL_BATCH_SIZE, with_labels=True\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Error creating validation dataloader: {e}\")\n",
    "    val_loader = None\n",
    "\n",
    "try:\n",
    "    test_loader = dataset_builder.create_dataloader(\n",
    "        data_type='test', batch_size=VAL_BATCH_SIZE, with_labels=False\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Error creating test dataloader: {e}\")\n",
    "    test_loader = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4364f9dd",
   "metadata": {},
   "source": [
    "#. Feature Extraction with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b06c0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tfidf_features(sentences_list, max_features=100, ngram_range=(1, 2,3)):\n",
    "    tfidf_vectorizer = TfidfVectorizer(\n",
    "        max_features=max_features, \n",
    "        ngram_range=ngram_range,\n",
    "        analyzer='char', \n",
    "        lowercase=False,\n",
    "        dtype=np.float32\n",
    "    )\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(sentences_list)\n",
    "    return tfidf_vectorizer, tfidf_matrix\n",
    "\n",
    "\n",
    "def convert_sequences_to_tfidf_features(sequences, char_to_index, max_len, tfidf_dim, device):\n",
    "    batch_size = sequences.shape[0]\n",
    "    tfidf_features = sequences.float().unsqueeze(-1) / float(len(char_to_index))\n",
    "    tfidf_features = tfidf_features.expand(batch_size, max_len, tfidf_dim)\n",
    "    \n",
    "    return tfidf_features.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ec3fe1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNWithTfidf(nn.Module):\n",
    "    def __init__(self, tfidf_dim, hidden_size, output_size, num_layers=2, dropout_rate=0.2):\n",
    "        super(RNNWithTfidf, self).__init__()\n",
    "        \n",
    "        self.tfidf_dim = tfidf_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.rnn = nn.RNN(\n",
    "            input_size=tfidf_dim,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=dropout_rate if num_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        \n",
    "        self.batchnorm = nn.BatchNorm1d(hidden_size * 2)\n",
    "        \n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        \n",
    "        self.output_layer = nn.Linear(hidden_size * 2, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        rnn_out, _ = self.rnn(x)  \n",
    "        rnn_out = self.dropout(rnn_out)\n",
    "        rnn_out = rnn_out.transpose(1, 2)  \n",
    "        rnn_out = self.batchnorm(rnn_out)\n",
    "        rnn_out = rnn_out.transpose(1, 2)  \n",
    "        output = self.output_layer(rnn_out)  \n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "model = RNNWithTfidf(\n",
    "    tfidf_dim=TFIDF_FEATURES,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    output_size=len(LABELS),\n",
    "    num_layers=NUM_LAYERS,\n",
    "    dropout_rate=DROPOUT_RATE\n",
    ").to(DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d23dd6e5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 119\u001b[39m\n\u001b[32m    114\u001b[39m         json.dump(metadata, f, ensure_ascii=\u001b[38;5;28;01mFalse\u001b[39;00m, indent=\u001b[32m4\u001b[39m)\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m model, history\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtrain_loader\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m val_loader \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    120\u001b[39m     model, training_history = train_model(\n\u001b[32m    121\u001b[39m         model=model,\n\u001b[32m    122\u001b[39m         train_loader=train_loader,\n\u001b[32m   (...)\u001b[39m\u001b[32m    127\u001b[39m         output_path=OUTPUT_PATH\n\u001b[32m    128\u001b[39m     )\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mNameError\u001b[39m: name 'train_loader' is not defined"
     ]
    }
   ],
   "source": [
    "def train_model(model, train_loader, val_loader, num_epochs=NUM_EPOCHS, \n",
    "                learning_rate=LEARNING_RATE, device=DEVICE, output_path=OUTPUT_PATH):\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=15)  \n",
    "    scheduler = StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "    \n",
    "    \n",
    "    history = {\n",
    "        'train_loss': [], 'train_acc': [], 'train_f1': [],\n",
    "        'val_loss': [], 'val_acc': [], 'val_f1': []\n",
    "    }\n",
    "    \n",
    "    best_f1 = -1\n",
    "    best_model_state = None\n",
    "        \n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        train_preds, train_trues = [], []\n",
    "        \n",
    "        for batch_sequences, batch_labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\", leave=False):\n",
    "            optimizer.zero_grad()\n",
    "            tfidf_features = convert_sequences_to_tfidf_features(\n",
    "                batch_sequences, CHAR_TO_INDEX, MAX_LENGTH, TFIDF_FEATURES, device\n",
    "            )\n",
    "            outputs = model(tfidf_features)\n",
    "            flat_outputs = outputs.view(-1, outputs.shape[-1])\n",
    "            flat_labels = batch_labels.view(-1)\n",
    "            mask = (flat_labels != 15)\n",
    "            loss = criterion(flat_outputs[mask], flat_labels[mask])\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            pred = flat_outputs.argmax(dim=1)\n",
    "            train_correct += (pred[mask] == flat_labels[mask]).sum().item()\n",
    "            train_total += mask.sum().item()\n",
    "            train_preds.extend(pred[mask].cpu().tolist())\n",
    "            train_trues.extend(flat_labels[mask].cpu().tolist())\n",
    "        train_loss /= len(train_loader)\n",
    "        train_accuracy = train_correct / train_total if train_total > 0 else 0\n",
    "        train_f1 = f1_score(train_trues, train_preds, average='macro', zero_division=0)\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        val_preds, val_trues = [], []\n",
    "        with torch.inference_mode():\n",
    "            for val_seq, val_label in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Val]\", leave=False):\n",
    "                tfidf_features = convert_sequences_to_tfidf_features(\n",
    "                    val_seq, CHAR_TO_INDEX, MAX_LENGTH, TFIDF_FEATURES, device\n",
    "                )\n",
    "                outputs = model(tfidf_features)\n",
    "                flat_outputs = outputs.view(-1, outputs.shape[-1])\n",
    "                flat_labels = val_label.view(-1)\n",
    "                loss = criterion(flat_outputs, flat_labels)\n",
    "                val_loss += loss.item()\n",
    "                pred = flat_outputs.argmax(dim=1)\n",
    "                mask = (flat_labels != 15)\n",
    "                val_correct += (pred[mask] == flat_labels[mask]).sum().item()\n",
    "                val_total += mask.sum().item()\n",
    "                val_preds.extend(pred[mask].cpu().tolist())\n",
    "                val_trues.extend(flat_labels[mask].cpu().tolist())\n",
    "        val_loss /= len(val_loader)\n",
    "        val_accuracy = val_correct / val_total if val_total > 0 else 0\n",
    "        val_f1 = f1_score(val_trues, val_preds, average='macro', zero_division=0)\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_accuracy)\n",
    "        history['train_f1'].append(train_f1)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_accuracy)\n",
    "        history['val_f1'].append(val_f1)\n",
    "        scheduler.step()\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        print(f\"  Train: Loss={train_loss:.4f}, Acc={train_accuracy*100:.2f}%, F1={train_f1:.3f}\")\n",
    "        print(f\"  Val:   Loss={val_loss:.4f}, Acc={val_accuracy*100:.2f}%, F1={val_f1:.3f}\")\n",
    "        \n",
    "        \n",
    "        if val_f1 > best_f1:\n",
    "            best_f1 = val_f1\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            print(f\"  ★ New best model saved (F1={best_f1:.3f})\")\n",
    "    \n",
    "    \n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    \n",
    "    \n",
    "    model_path = os.path.join(output_path, 'rnn_tfidf_model.pth')\n",
    "    meta_path = os.path.join(output_path, 'rnn_tfidf_model_meta.json')\n",
    "    \n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    \n",
    "    metadata = {\n",
    "        'model_type': 'RNNWithTfidf',\n",
    "        'best_val_f1': float(best_f1),\n",
    "        'tfidf_dim': TFIDF_FEATURES,\n",
    "        'hidden_size': HIDDEN_SIZE,\n",
    "        'num_layers': NUM_LAYERS,\n",
    "        'dropout_rate': DROPOUT_RATE,\n",
    "        'learning_rate': LEARNING_RATE,\n",
    "        'num_epochs': num_epochs,\n",
    "        'max_sequence_length': MAX_LENGTH,\n",
    "        'train_batch_size': TRAIN_BATCH_SIZE,\n",
    "        'vocab_size': len(CHAR_TO_INDEX) + 1,\n",
    "        'output_classes': len(LABELS)\n",
    "    }\n",
    "    \n",
    "    with open(meta_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(metadata, f, ensure_ascii=False, indent=4)\n",
    "    return model, history\n",
    "\n",
    "\n",
    "\n",
    "if train_loader is not None and val_loader is not None:\n",
    "    model, training_history = train_model(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        num_epochs=NUM_EPOCHS,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        device=DEVICE,\n",
    "        output_path=OUTPUT_PATH\n",
    "    )\n",
    "else:\n",
    "    print(\"Error: Could not create train/val loaders. Skipping training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1093e34",
   "metadata": {},
   "source": [
    "#. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aced031c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 34\u001b[39m\n\u001b[32m     23\u001b[39m     weighted_f1 = f1_score(all_trues, all_preds, average=\u001b[33m'\u001b[39m\u001b[33mweighted\u001b[39m\u001b[33m'\u001b[39m, zero_division=\u001b[32m0\u001b[39m)\n\u001b[32m     24\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m     25\u001b[39m         \u001b[33m'\u001b[39m\u001b[33maccuracy\u001b[39m\u001b[33m'\u001b[39m: accuracy,\n\u001b[32m     26\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mmacro_f1\u001b[39m\u001b[33m'\u001b[39m: macro_f1,\n\u001b[32m   (...)\u001b[39m\u001b[32m     29\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mground_truth\u001b[39m\u001b[33m'\u001b[39m: all_trues\n\u001b[32m     30\u001b[39m     }\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtest_loader\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     35\u001b[39m     test_results = evaluate_model(model, test_loader, device=DEVICE)\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mNameError\u001b[39m: name 'test_loader' is not defined"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, test_loader, device=DEVICE):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_trues = []\n",
    "    correct = 0\n",
    "    total = 0 \n",
    "    with torch.inference_mode():\n",
    "        for test_seq, test_label in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "            tfidf_features = convert_sequences_to_tfidf_features(\n",
    "                test_seq, CHAR_TO_INDEX, MAX_LENGTH, TFIDF_FEATURES, device\n",
    "            )\n",
    "            outputs = model(tfidf_features)\n",
    "            predictions = outputs.argmax(dim=2)\n",
    "            flat_preds = predictions.view(-1)\n",
    "            flat_trues = test_label.view(-1)\n",
    "            mask = (flat_trues != 15) & (flat_trues != 16)\n",
    "            correct += (flat_preds[mask] == flat_trues[mask]).sum().item()\n",
    "            total += mask.sum().item()\n",
    "            all_preds.extend(flat_preds[mask].cpu().numpy())\n",
    "            all_trues.extend(flat_trues[mask].cpu().numpy())\n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    macro_f1 = f1_score(all_trues, all_preds, average='macro', zero_division=0)\n",
    "    weighted_f1 = f1_score(all_trues, all_preds, average='weighted', zero_division=0)\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'macro_f1': macro_f1,\n",
    "        'weighted_f1': weighted_f1,\n",
    "        'predictions': all_preds,\n",
    "        'ground_truth': all_trues\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "if test_loader is not None:\n",
    "    test_results = evaluate_model(model, test_loader, device=DEVICE)\n",
    "else:\n",
    "    print(\"Error:Test loader not available. Skipping evaluation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140f8b50",
   "metadata": {},
   "source": [
    "#. Model Loading and Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee304572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ Warning: Test data not found at data/sample_test_no_diacritics.txt\n"
     ]
    }
   ],
   "source": [
    "def load_trained_model(model_path, device=DEVICE):\n",
    "    model = RNNWithTfidf(\n",
    "        input_size=TFIDF_FEATURES,\n",
    "        hidden_size=HIDDEN_SIZE,\n",
    "        num_layers=NUM_LAYERS,\n",
    "        output_size=len(LABELS),\n",
    "        dropout=DROPOUT_RATE\n",
    "    )\n",
    "    \n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "def predict_on_test_set(model, test_data_path, output_csv_path=None):\n",
    "    model.eval()\n",
    "    \n",
    "    \n",
    "    with open(test_data_path, 'r', encoding='utf-8') as f:\n",
    "        test_sentences = [line.strip() for line in f.readlines()]\n",
    "    \n",
    "    predictions_list = []\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for sentence in tqdm(test_sentences, desc=\"Predicting\"):\n",
    "            \n",
    "            cleaned = text_cleaner.clean_text(sentence)\n",
    "            sequences = text_preprocessor.preprocess_text(cleaned)\n",
    "            \n",
    "            \n",
    "            padded_seq = sequences + [15] * (MAX_LENGTH - len(sequences))\n",
    "            padded_seq = padded_seq[:MAX_LENGTH]\n",
    "            \n",
    "            \n",
    "            seq_tensor = torch.tensor([padded_seq], dtype=torch.long, device=DEVICE)\n",
    "            \n",
    "            tfidf_features = convert_sequences_to_tfidf_features(\n",
    "                seq_tensor, CHAR_TO_INDEX, MAX_LENGTH, TFIDF_FEATURES, DEVICE\n",
    "            )\n",
    "            \n",
    "            \n",
    "            with torch.inference_mode():\n",
    "                outputs = model(tfidf_features)\n",
    "                pred_indices = outputs.argmax(dim=2)[0].cpu().numpy()\n",
    "            \n",
    "            \n",
    "            diacritics = [LABELS.get(idx, '') for idx in pred_indices]\n",
    "            \n",
    "            \n",
    "            diacritized = ''\n",
    "            for i, char in enumerate(cleaned):\n",
    "                if i < len(diacritics):\n",
    "                    diacritized += char + diacritics[i]\n",
    "                else:\n",
    "                    diacritized += char\n",
    "            \n",
    "            predictions_list.append({\n",
    "                'original': sentence,\n",
    "                'cleaned': cleaned,\n",
    "                'diacritized': diacritized\n",
    "            })\n",
    "    df = pd.DataFrame(predictions_list)\n",
    "    if output_csv_path is None:\n",
    "        output_csv_path = os.path.join(OUTPUT_PATH, 'test_predictions.csv')\n",
    "    df.to_csv(output_csv_path, index=False, encoding='utf-8')\n",
    "    return df\n",
    "\n",
    "TEST_DATA_PATH=\"data/sample_test_no_diacritics.txt\"\n",
    "\n",
    "if os.path.exists(TEST_DATA_PATH):\n",
    "    test_predictions = predict_on_test_set(model, TEST_DATA_PATH)\n",
    "    print(\"\\nFirst few predictions:\")\n",
    "    print(test_predictions.head())\n",
    "else:\n",
    "    print(f\"Error: Test data not found at {TEST_DATA_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a83b85",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
