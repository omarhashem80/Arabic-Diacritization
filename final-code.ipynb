{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "sourceId": 13885877,
     "sourceType": "datasetVersion",
     "datasetId": 8846952
    },
    {
     "sourceId": 14053250,
     "sourceType": "datasetVersion",
     "datasetId": 8945448
    },
    {
     "sourceId": 14071327,
     "sourceType": "datasetVersion",
     "datasetId": 8903667
    },
    {
     "sourceId": 14072039,
     "sourceType": "datasetVersion",
     "datasetId": 8957446
    },
    {
     "sourceId": 677572,
     "sourceType": "modelInstanceVersion",
     "isSourceIdPinned": true,
     "modelInstanceId": 513809,
     "modelId": 528448
    }
   ],
   "dockerImageVersionId": 31193,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": "import re\n\n\nclass TextCleaner:\n    @staticmethod\n    def replace_regex(text, pattern, replacement=''):\n        return pattern.sub(replacement, text)\n\n    @staticmethod\n    def clean_lines(lines):\n        for i in range(len(lines)):\n            # remove any brackets that have only numbers inside and remove all numbers\n            reg = r'\\(\\s*(\\d+)\\s*\\)|\\(\\s*(\\d+)\\s*\\/\\s*(\\d+)\\s*\\)|\\d+'\n            lines[i] = TextCleaner.replace_regex(lines[i], re.compile(reg))\n            # replace all different types of brackets with a single type\n            reg_brackets = r'[\\[\\{\\(\\]\\}\\)]'\n            lines[i] = re.compile(reg_brackets).sub('', lines[i])\n            # remove some unwanted characters\n            #reg = r'[/!\\-؛،؟:\\.]'\n            reg = r'[/\\/\\\\\\-]'\n            lines[i] = TextCleaner.replace_regex(lines[i], re.compile(reg))\n            # remove unwanted characters\n            reg = r'[,»–\\';«*\\u200f\\u200d\\u200b\\u200c\\u200e\"\\\\~`%…_]'\n            lines[i] = TextCleaner.replace_regex(lines[i], re.compile(reg))\n            # remove English characters (a-z, A-Z)\n            reg = r'[a-zA-Z]'\n            lines[i] = TextCleaner.replace_regex(lines[i], re.compile(reg))\n            # remove fractions and superscripts/subscripts\n            reg = r'[\\u00BC-\\u00BE\\u2150-\\u215E]'\n            lines[i] = TextCleaner.replace_regex(lines[i], re.compile(reg))\n            # remove emojis and other symbols\n            reg = r'[\\U0001F000-\\U0001FFFF]'\n            lines[i] = TextCleaner.replace_regex(lines[i], re.compile(reg))\n            # remove gender symbols and similar miscellaneous symbols\n            reg = r'[\\u2600-\\u26FF\\u2700-\\u27BF]'\n            lines[i] = TextCleaner.replace_regex(lines[i], re.compile(reg))\n            # remove extra spaces\n            reg = r'\\s+'\n            lines[i] = TextCleaner.replace_regex(lines[i], re.compile(reg), ' ')\n        return lines\n\n    @staticmethod\n    def remove_diacritics(lines):\n        diacritics_pattern = r'[\\u064B-\\u065F\\u0670\\uFE70-\\uFE7F]'\n        for i in range(len(lines)):\n            lines[i] = TextCleaner.replace_regex(lines[i], re.compile(diacritics_pattern))\n        return lines\n\n    @staticmethod\n    def count_spaces(text):\n        return len(re.findall(r'\\s', text))",
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-09T23:19:36.714674Z",
     "iopub.execute_input": "2025-12-09T23:19:36.714948Z",
     "iopub.status.idle": "2025-12-09T23:19:36.737854Z",
     "shell.execute_reply.started": "2025-12-09T23:19:36.714932Z",
     "shell.execute_reply": "2025-12-09T23:19:36.737272Z"
    }
   },
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "source": "import re\nimport textwrap\n\n\nclass TextPreprocessor:\n    def __init__(self, cleaner, input_path='.', output_path='.', max_length=600, with_labels=True):\n        self.cleaner = cleaner\n        self.input_path = input_path\n        self.output_path = output_path\n        self.max_length = max_length\n        self.with_labels = with_labels\n\n    def clean_lines(self, lines, data_type):\n        lines = self.cleaner.clean_lines(lines)\n\n        if not lines:\n            return lines\n\n        # Save lines with diacritics\n        if self.with_labels:\n            with open(f'{self.output_path}cleaned_{data_type}_with_diacritics.txt', 'a+', encoding='utf-8') as f:\n                f.write('\\n'.join(lines) + '\\n')\n\n        # Remove diacritics\n        lines_no_diacritics = self.cleaner.remove_diacritics(lines)\n\n        # Save lines without diacritics\n        with open(f'{self.output_path}cleaned_{data_type}_without_diacritics.txt', 'a+', encoding='utf-8') as f:\n            f.write('\\n'.join(lines_no_diacritics) + '\\n')\n\n        return lines_no_diacritics\n\n    def preprocess_file(self, data_type, limit=None):\n        # Clear previous output\n        open(f'{self.output_path}cleaned_{data_type}_with_diacritics.txt', 'w', encoding='utf-8').close()\n        open(f'{self.output_path}cleaned_{data_type}_without_diacritics.txt', 'w', encoding='utf-8').close()\n\n        # Read raw file\n        with open(f'{self.input_path}{data_type}.txt', 'r', encoding='utf-8') as f:\n            lines = [line.strip() for line in f.readlines()]\n\n        if limit is not None:\n            lines = lines[:limit]\n\n        return self.clean_lines(lines, data_type)\n\n    def tokenize_file(self, data_type):\n        tokenized_no_diacritics = []\n        space_counts = []\n\n        # Tokenize file without diacritics\n        with open(f'{self.output_path}cleaned_{data_type}_without_diacritics.txt', 'r', encoding='utf-8') as f:\n            for line in f:\n                line = re.sub(r'[\\n\\r\\t]', '', line)\n                line = re.sub(r'\\s+', ' ', line).strip()\n                sentences = textwrap.wrap(line, self.max_length)\n                for sentence in sentences:\n                    tokenized_no_diacritics.append(sentence)\n                    space_counts.append(self.cleaner.count_spaces(sentence))\n\n        tokenized_with_diacritics = []\n\n        if self.with_labels:\n            space_index = 0\n            with open(f'{self.output_path}cleaned_{data_type}_with_diacritics.txt', 'r', encoding='utf-8') as f:\n                for line in f:\n                    line = re.sub(r'[\\n\\r\\t]', '', line)\n                    line = re.sub(r'\\s+', ' ', line).strip()\n                    remaining_text = line\n                    while remaining_text:\n                        spaces_to_include = space_counts[space_index]\n                        space_index += 1\n                        words = remaining_text.split()\n                        if len(words) <= spaces_to_include + 1:\n                            tokenized_with_diacritics.append(remaining_text.strip())\n                            break\n                        sentence = ' '.join(words[:spaces_to_include + 1])\n                        tokenized_with_diacritics.append(sentence.strip())\n                        remaining_text = ' '.join(words[spaces_to_include + 1:]).strip()\n\n        return tokenized_no_diacritics, tokenized_with_diacritics\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-09T23:19:36.794164Z",
     "iopub.execute_input": "2025-12-09T23:19:36.794386Z",
     "iopub.status.idle": "2025-12-09T23:19:36.806627Z",
     "shell.execute_reply.started": "2025-12-09T23:19:36.794369Z",
     "shell.execute_reply": "2025-12-09T23:19:36.805833Z"
    }
   },
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "source": "import torch\nfrom torch.utils.data import DataLoader, TensorDataset\n\n\nclass DatasetBuilder:\n    def __init__(self, preprocessor, char_to_index, label_map, max_length=600, device='cpu'):\n        self.preprocessor = preprocessor\n        self.char_to_index = char_to_index\n        self.label_map = label_map\n        self.max_length = max_length\n        self.device = device\n\n    def encode_chars(self, sequences):\n        # Convert characters to indices and pad sequences\n        indexed = [[self.char_to_index[char] for char in seq] for seq in sequences]\n        padded = [seq + [0] * (self.max_length - len(seq)) for seq in indexed]\n        return torch.tensor(padded).to(self.device)\n\n    def encode_labels(self, sequences_with_diacritics):\n        # Convert diacritics to label indices and pad sequences\n        all_labels = []\n        for sentence in sequences_with_diacritics:\n            sentence_labels = []\n            i = 0\n            while i < len(sentence):\n                char_code = ord(sentence[i])\n                if char_code not in self.label_map:\n                    if (i + 1 < len(sentence)) and ord(sentence[i + 1]) in self.label_map:\n                        diacritic = ord(sentence[i + 1])\n                        if diacritic == 1617 and (i + 2 < len(sentence)) and ord(sentence[i + 2]) in self.label_map:\n                            sentence_labels.append(self.label_map[(1617, ord(sentence[i + 2]))])\n                            i += 3\n                            continue\n                        elif diacritic == 1617:\n                            sentence_labels.append(self.label_map[1617])\n                            i += 2\n                            continue\n                        else:\n                            sentence_labels.append(self.label_map[diacritic])\n                            i += 2\n                            continue\n                    else:\n                        sentence_labels.append(14)  # No diacritic\n                i += 1\n            all_labels.append(sentence_labels)\n\n        # Pad label sequences\n        padded_labels = [seq + [15] * (self.max_length - len(seq)) for seq in all_labels]\n        return torch.tensor(padded_labels).to(self.device)\n\n    def create_dataloader(self, data_type, batch_size=256, with_labels=True):\n        # Preprocess and tokenize\n        self.preprocessor.preprocess_file(data_type)\n        sequences, sequences_with_diacritics = self.preprocessor.tokenize_file(data_type)\n\n        # Encode characters\n        data_tensor = self.encode_chars(sequences)\n\n        # Encode labels\n        if with_labels:\n            labels_tensor = self.encode_labels(sequences_with_diacritics)\n        else:\n            labels_tensor = torch.tensor([[15] * self.max_length] * len(data_tensor)).to(self.device)\n\n        dataset = TensorDataset(data_tensor, labels_tensor)\n        return DataLoader(dataset, batch_size=batch_size)\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-09T23:19:36.807986Z",
     "iopub.execute_input": "2025-12-09T23:19:36.808323Z",
     "iopub.status.idle": "2025-12-09T23:19:36.836132Z",
     "shell.execute_reply.started": "2025-12-09T23:19:36.808297Z",
     "shell.execute_reply": "2025-12-09T23:19:36.835396Z"
    }
   },
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "source": "import torch\nimport torch.nn as nn\nfrom torch.nn import BatchNorm1d\nimport pickle\nimport json\n\n\nclass CharBiLSTM(nn.Module):\n    def __init__(self, vocab_size=44, embedding_dim=300, hidden_dim=256, output_dim=16,\n                 dropout_rate=0.2, num_layers=1, max_length=600):\n        super(CharBiLSTM, self).__init__()\n\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.lstm = nn.LSTM(\n            input_size=embedding_dim,\n            hidden_size=hidden_dim,\n            num_layers=num_layers,\n            batch_first=True,\n            bidirectional=True,\n            dropout=dropout_rate,\n        )\n        self.batchnorm = nn.BatchNorm1d(max_length)\n        self.output = nn.Linear(hidden_dim * 2, output_dim)\n\n    def forward(self, x):\n        x_embed = self.embedding(x)\n        lstm_out, _ = self.lstm(x_embed)\n        lstm_out = self.batchnorm(lstm_out)\n        out = self.output(lstm_out)\n        return out\n\n    @staticmethod\n    def load_model(model_file=\"best_model.pkl\", meta_file=\"best_model_meta.json\", device=None):\n        if device is None:\n            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n        base_path = '/kaggle/working/'\n\n        with open(base_path + meta_file, \"r\", encoding=\"utf-8\") as f:\n            metadata = json.load(f)\n\n        model = CharBiLSTM(\n            vocab_size=metadata[\"vocab_size\"],\n            embedding_dim=metadata[\"embedding_size\"],\n            hidden_dim=metadata[\"hidden_size\"],\n            output_dim=metadata[\"output_classes\"],\n            dropout_rate=metadata[\"dropout_rate\"],\n            num_layers=metadata[\"num_layers\"],\n            max_length=metadata[\"max_sequence_length\"]\n        ).to(device)\n\n        # # ---- Load best weights ----\n        # state_dict = torch.load(base_path + model_file, map_location=device)\n        # model.load_state_dict(state_dict)\n        with open(base_path + model_file, \"rb\") as f:\n            state_dict = pickle.load(f)\n\n        model.load_state_dict(state_dict)\n        model.to(device)\n\n        model.eval()\n        return model, metadata",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-09T23:21:25.613663Z",
     "iopub.execute_input": "2025-12-09T23:21:25.614252Z",
     "iopub.status.idle": "2025-12-09T23:21:25.622146Z",
     "shell.execute_reply.started": "2025-12-09T23:21:25.614233Z",
     "shell.execute_reply": "2025-12-09T23:21:25.621541Z"
    }
   },
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "source": "import torch\nfrom torch.optim.lr_scheduler import StepLR\nfrom tqdm import tqdm\nimport json\nfrom sklearn.metrics import f1_score\n# from models import CharBiLSTM  \n\n\nclass Trainer:\n    def __init__(\n        self, model, optimizer=None, scheduler=None, criterion=None,\n        train_loader=None, val_loader=None,\n        pad_idx=15, max_length=600, device=None,\n        checkpoint_file=\"checkpoint.pth\", best_model_file=\"best_model.pth\",\n        meta_file=\"best_model_meta.json\"\n    ):\n        self.device = device or torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.model = model.to(self.device)\n\n        self.optimizer = optimizer\n        self.scheduler = scheduler\n        self.criterion = criterion\n        self.train_loader = train_loader\n        self.val_loader = val_loader\n\n        self.pad_idx = pad_idx\n        self.max_length = max_length\n\n        self.checkpoint_file = checkpoint_file\n        self.best_model_file = best_model_file\n        self.meta_file = meta_file\n\n        self.best_val_acc = -1   \n\n        # Save metadata once\n        self._save_metadata()\n\n    def _save_metadata(self):\n        meta = {\n            \"vocab_size\": self.model.vocab_size,\n            \"embedding_size\": self.model.embedding.embedding_dim,\n            \"hidden_size\": self.model.hidden_size,\n            \"output_classes\": self.model.output_size,\n            \"dropout_rate\": self.model.dropout_rate,\n            \"num_layers\": self.model.num_layers,\n            \"max_sequence_length\": self.max_length,\n            \"learning_rate\": self.optimizer.param_groups[0][\"lr\"],\n            \"pad_idx\": self.pad_idx,\n        }\n\n        with open(self.meta_file, \"w\", encoding=\"utf-8\") as f:\n            json.dump(meta, f, indent=4)\n\n    @staticmethod\n    def load_checkpoint(\n        checkpoint_file=\"checkpoint.pth\",\n        meta_file=\"best_model_meta.json\",\n        model=None,\n        device=None,\n    ):\n        device = device or torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n        checkpoint = torch.load(checkpoint_file, map_location=device)\n\n        with open(meta_file, \"r\", encoding=\"utf-8\") as f:\n            meta = json.load(f)\n\n        if model is None:\n            model = CharBiLSTM(\n                vocab_size=meta[\"vocab_size\"],\n                embedding_dim=meta[\"embedding_size\"],\n                hidden_dim=meta[\"hidden_size\"],\n                output_dim=meta[\"output_classes\"],\n                dropout_rate=meta[\"dropout_rate\"],\n                num_layers=meta[\"num_layers\"],\n                max_length=meta[\"max_sequence_length\"]\n            ).to(device)\n\n        optimizer = torch.optim.Adam(model.parameters(), lr=meta[\"learning_rate\"])\n        scheduler = StepLR(optimizer, step_size=5, gamma=0.1)\n\n        model.load_state_dict(checkpoint[\"model_state\"])\n        optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\n\n        if checkpoint.get(\"scheduler_state\") is not None:\n            scheduler.load_state_dict(checkpoint[\"scheduler_state\"])\n\n        start_epoch = checkpoint[\"epoch\"] + 1\n        best_val_acc = checkpoint[\"best_val_acc\"]  \n\n        return model, optimizer, scheduler, start_epoch, best_val_acc\n\n    def train_epoch(self):\n        self.model.train()\n        correct, total = 0, 0\n        preds, trues = [], []\n\n        for batch_seq, batch_labels in self.train_loader:\n            batch_seq, batch_labels = batch_seq.to(self.device), batch_labels.to(self.device)\n            self.optimizer.zero_grad()\n\n            outputs = self.model(batch_seq)\n\n            flat_outputs = outputs.view(-1, outputs.shape[-1])\n            flat_labels = batch_labels.view(-1)\n            mask = (flat_labels != self.pad_idx)\n\n            loss = self.criterion(flat_outputs[mask], flat_labels[mask])\n            loss.backward()\n            self.optimizer.step()\n\n            pred = flat_outputs.argmax(dim=1)\n            correct += (pred[mask] == flat_labels[mask]).sum().item()\n            total += mask.sum().item()\n\n            preds.extend(pred[mask].cpu().tolist())\n            trues.extend(flat_labels[mask].cpu().tolist())\n\n        acc = correct / total\n        f1 = f1_score(trues, preds, average='macro')\n        return acc, f1, loss.item()\n\n    def validate(self):\n        self.model.eval()\n        correct, total = 0, 0\n        preds, trues = [], []\n\n        with torch.inference_mode():\n            for batch_seq, batch_labels in self.val_loader:\n                batch_seq, batch_labels = batch_seq.to(self.device), batch_labels.to(self.device)\n                outputs = self.model(batch_seq)\n\n                pred = outputs.argmax(dim=2)\n                flat_pred = pred.view(-1)\n                flat_labels = batch_labels.view(-1)\n                mask = (flat_labels != self.pad_idx)\n\n                correct += (flat_pred[mask] == flat_labels[mask]).sum().item()\n                total += mask.sum().item()\n\n                preds.extend(flat_pred[mask].cpu().tolist())\n                trues.extend(flat_labels[mask].cpu().tolist())\n\n        acc = correct / total\n        f1 = f1_score(trues, preds, average='macro')\n        return acc, f1\n\n    def train(self, num_epochs=20):\n        print(\"Training...\")\n\n        for epoch in range(num_epochs):\n            train_acc, train_f1, loss_val = self.train_epoch()\n            val_acc, val_f1 = self.validate()\n\n            if self.scheduler:\n                self.scheduler.step()\n\n            print(\n                f\"Epoch {epoch+1}/{num_epochs} | Loss: {loss_val:.5f} | \"\n                f\"Train Acc: {train_acc*100:.2f}% | Train F1: {train_f1:.3f} | \"\n                f\"Val Acc: {val_acc*100:.2f}% | Val F1: {val_f1:.3f}\"\n            )\n\n            # SAVE BEST MODEL (based on val_acc)\n            if val_acc > self.best_val_acc:\n                self.best_val_acc = val_acc\n                torch.save(self.model.state_dict(), self.best_model_file)\n                print(f\"New best model saved! (Val Acc = {val_acc*100:.2f}%)\")\n\n            # SAVE LAST CHECKPOINT\n            torch.save({\n                \"model_state\": self.model.state_dict(),\n                \"optimizer_state\": self.optimizer.state_dict(),\n                \"scheduler_state\": self.scheduler.state_dict() if self.scheduler else None,\n                \"best_val_acc\": self.best_val_acc,   \n                \"epoch\": epoch\n            }, self.checkpoint_file)\n\n        return self.best_val_acc\n\n    def test(self, test_loader):\n        self.model.eval()\n        correct, total = 0, 0\n\n        with torch.inference_mode():\n            for batch_seq, batch_labels in tqdm(test_loader, desc=\"Testing\"):\n                batch_seq, batch_labels = batch_seq.to(self.device), batch_labels.to(self.device)\n                outputs = self.model(batch_seq)\n\n                pred_labels = outputs.argmax(dim=2)\n                mask = (batch_labels != self.pad_idx)\n\n                correct += ((pred_labels == batch_labels) & mask).sum().item()\n                total += mask.sum().item()\n\n        acc = correct / total if total > 0 else 0\n        print(f\"Test Accuracy: {acc*100:.3f}%\")\n        return acc\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-09T23:19:36.862255Z",
     "iopub.execute_input": "2025-12-09T23:19:36.862466Z",
     "iopub.status.idle": "2025-12-09T23:19:36.889833Z",
     "shell.execute_reply.started": "2025-12-09T23:19:36.862444Z",
     "shell.execute_reply": "2025-12-09T23:19:36.888999Z"
    }
   },
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "source": "import torch\nimport re\nimport textwrap\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom tqdm import tqdm\n\n\nclass Predictor:\n    def __init__(self, model, char_to_index, index_to_label, device=None):\n        self.model = model\n        self.char_to_index = char_to_index\n        self.index_to_label = index_to_label\n        self.device = device or torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.model.to(self.device)\n\n    def predict_sentence(self, sentence, max_length=200, batch_size=256):\n        # Preprocess and tokenize the sentence\n        cleaned = re.sub(r'\\s+', ' ', sentence.strip())\n        tokenized = textwrap.wrap(cleaned, max_length)\n\n        sequences = [\n            [self.char_to_index.get(c, 0) for c in s] + [0] * (max_length - len(s))\n            for s in tokenized\n        ]\n        sequences_tensor = torch.tensor(sequences).to(self.device)\n\n        dataset = TensorDataset(sequences_tensor, sequences_tensor)\n        dataloader = DataLoader(dataset, batch_size=batch_size)\n        predicted_labels = []\n\n        self.model.eval()\n        with torch.inference_mode():\n            for batch_seq, _ in dataloader:\n                outputs = self.model(batch_seq)\n                batch_pred = outputs.argmax(dim=2)\n                mask = batch_seq != 0\n                predicted_labels.extend(batch_pred[mask].tolist())\n\n        # Reconstruct sentence with diacritics\n        reconstructed = \"\"\n        idx = 0\n        for char in sentence:\n            reconstructed += char\n            if char not in self.char_to_index or self.char_to_index[char] in [0, 2, 8, 15, 16, 26, 40, 43]:\n                continue\n            predicted_class = self.index_to_label[predicted_labels[idx]]\n            if isinstance(predicted_class, tuple):\n                reconstructed += chr(predicted_class[0]) + chr(predicted_class[1])\n            elif predicted_class != 0:\n                reconstructed += chr(predicted_class)\n            idx += 1\n\n        return reconstructed\n\n    def predict_dataset(self, dataloader):\n        # Predict labels for a full dataset and save to submission.csv\n        predicted_labels = []\n\n        self.model.eval()\n        with torch.inference_mode():\n            for batch_seq, _ in dataloader:\n                batch_seq = batch_seq.to(self.device)\n                outputs = self.model(batch_seq)\n                batch_pred = outputs.argmax(dim=2)\n\n                mask = (batch_seq != 0) & (batch_seq != 2) & (batch_seq != 8) & \\\n                       (batch_seq != 15) & (batch_seq != 16) & (batch_seq != 26) & \\\n                       (batch_seq != 40) & (batch_seq != 43)\n\n                predicted_labels.extend(batch_pred[mask].tolist())\n\n        # Save predictions\n        with open('submission.csv', 'w', encoding='utf-8') as f:\n            f.write('ID,label\\n')\n            for i, label in enumerate(predicted_labels):\n                f.write(f\"{i},{label}\\n\")\n\n        return predicted_labels\n        \n    def evaluate(self, test_loader):\n        self.model.eval()\n        correct, total = 0, 0\n    \n        with torch.inference_mode():\n            for seq, labels in tqdm(test_loader, desc=\"Evaluating\"):\n                seq = seq.to(self.device)\n                labels = labels.to(self.device)\n    \n                outputs = self.model(seq)\n                pred = outputs.argmax(dim=2)\n    \n                flat_pred = pred.view(-1)\n                flat_labels = labels.view(-1)\n    \n                # mask out PAD = 15\n                mask = (flat_labels != 15)\n    \n                correct += (flat_pred[mask] == flat_labels[mask]).sum().item()\n                total += mask.sum().item()\n    \n        return correct / total if total > 0 else 0\n\n\n    # def evaluate(self, test_loader):\n    #     # Evaluate model accuracy on a test/validation set\n    #     self.model.eval()\n    #     correct, total = 0, 0\n\n    #     with torch.inference_mode():\n    #         for batch_seq, batch_labels in tqdm(test_loader, desc=\"Evaluating\"):\n    #             batch_seq, batch_labels = batch_seq.to(self.device), batch_labels.to(self.device)\n    #             outputs = self.model(batch_seq)\n    #             pred_labels = outputs.argmax(dim=2)\n\n    #             mask = (batch_labels != 15) & (batch_seq != 2) & (batch_seq != 8) & \\\n    #                    (batch_seq != 15) & (batch_seq != 16) & (batch_seq != 26) & \\\n    #                    (batch_seq != 40) & (batch_seq != 43)\n\n    #             correct += ((pred_labels == batch_labels) & mask).sum().item()\n    #             total += mask.sum().item()\n    #     print(correct)\n    #     accuracy = correct / total if total > 0 else 0\n    #     print(f\"Test Accuracy: {accuracy * 100:.3f}%\")\n    #     return accuracy\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-09T23:19:36.891510Z",
     "iopub.execute_input": "2025-12-09T23:19:36.891842Z",
     "iopub.status.idle": "2025-12-09T23:19:36.921206Z",
     "shell.execute_reply.started": "2025-12-09T23:19:36.891824Z",
     "shell.execute_reply": "2025-12-09T23:19:36.920495Z"
    }
   },
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "source": "import torch\nfrom torch.optim.lr_scheduler import StepLR\n# from cleaner import TextCleaner\n# from preprocessor import TextPreprocessor\n# from dataset_builder import DatasetBuilder\n# from models import CharBiLSTM\n# from trainer import Trainer\n# from predictor import Predictor\n\n# -------------------- CONFIG --------------------\n\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nCHAR_TO_INDEX = {\n    'د': 1, '؟': 2, 'آ': 3, 'إ': 4, 'ؤ': 5, 'ط': 6, 'م': 7, '،': 8, 'ة': 9, 'ت': 10,\n    'ر': 11, 'ئ': 12, 'ا': 13, 'ض': 14, '!': 15, ' ': 16, 'ك': 17, 'غ': 18, 'س': 19,\n    'ص': 20, 'أ': 21, 'ل': 22, 'ف': 23, 'ظ': 24, 'ج': 25, '؛': 26, 'ن': 27, 'ع': 28,\n    'ب': 29, 'ث': 30, 'ه': 31, 'خ': 32, 'ى': 33, 'ء': 34, 'ز': 35, 'ق': 36, 'ي': 37,\n    'ش': 38, 'ح': 39, ':': 40, 'ذ': 41, 'و': 42, '.': 43\n}\nINDEX_TO_CHAR = {v: k for k, v in CHAR_TO_INDEX.items()}\n\nLABELS = {\n    1614: 0, 1611: 1, 1615: 2, 1612: 3, 1616: 4, 1613: 5, 1618: 6, 1617: 7,\n    (1617, 1614): 8, (1617, 1611): 9, (1617, 1615): 10, (1617, 1612): 11,\n    (1617, 1616): 12, (1617, 1613): 13, 0: 14, 15: 15\n}\nINDEX_TO_LABEL = {v: k for k, v in LABELS.items()}\n\nMAX_LENGTH = 600\nTRAIN_BATCH_SIZE = 32\nVAL_BATCH_SIZE = 256\nREAD_PATH = \"/kaggle/input/nlp-project-data/data/\"\nWRITE_PATH = \"/kaggle/working/\"\n\n# -------------------- MAIN --------------------\n\nif __name__ == \"__main__\":\n    # 1. Initialize preprocessing classes\n    cleaner = TextCleaner()\n    preprocessor = TextPreprocessor(cleaner, input_path=READ_PATH, output_path=WRITE_PATH)\n    dataset_builder = DatasetBuilder(preprocessor, char_to_index=CHAR_TO_INDEX, label_map=LABELS,\n                                     max_length=MAX_LENGTH, device=DEVICE)\n\n    # 2. Prepare dataloaders\n    # train_loader = dataset_builder.create_dataloader(data_type='train', batch_size=TRAIN_BATCH_SIZE)\n    # val_loader = dataset_builder.create_dataloader(data_type='val', batch_size=VAL_BATCH_SIZE)\n    # test_loader = dataset_builder.create_dataloader(data_type='test', batch_size=VAL_BATCH_SIZE, with_labels=False)\n\n    # # 3. Initialize model\n    # vocab_size = len(CHAR_TO_INDEX) + 1\n    # embedding_dim = 300\n    # hidden_dim = 256\n    # output_dim = len(LABELS)\n    # dropout_rate = 0.2\n    # num_layers = 5\n\n    # model = CharBiLSTM(vocab_size, embedding_dim, hidden_dim, output_dim, dropout_rate, num_layers,\n    #                    max_length=MAX_LENGTH).to(DEVICE)\n    # optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n    # scheduler = StepLR(optimizer, step_size=5, gamma=0.1)\n\n    # # 4. Train the model\n    # criterion = torch.nn.CrossEntropyLoss()\n    # trainer = Trainer(model=model, optimizer=optimizer, scheduler=scheduler, criterion=criterion,\n    #                   train_loader=train_loader, val_loader=val_loader, device=DEVICE,\n    #                   checkpoint_file=\"checkpoint.pth\")\n    # trainer.train(num_epochs=20)\n    \n    # # 5. Load best model for prediction\n    # model.load_state_dict(best_model_state)\n    # predictor = Predictor(model, CHAR_TO_INDEX, INDEX_TO_LABEL, device=DEVICE)\n\n    # 6. Predict on test dataset\n    # predictor.predict_dataset(test_loader)\n\n    # 7. Predict a single sentence\n    # test_sentence = ''\n    # predicted_sentence = predictor.predict_sentence(test_sentence, max_length=MAX_LENGTH, batch_size=VAL_BATCH_SIZE)\n    # print(\"Original sentence:\", test_sentence)\n    # print(\"Predicted sentence:\", predicted_sentence)\n\n    # 8. Evaluate on validation set\n    # predictor.evaluate(val_loader)\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-09T23:19:36.922016Z",
     "iopub.execute_input": "2025-12-09T23:19:36.922367Z",
     "iopub.status.idle": "2025-12-09T23:19:36.947366Z",
     "shell.execute_reply.started": "2025-12-09T23:19:36.922347Z",
     "shell.execute_reply": "2025-12-09T23:19:36.946597Z"
    }
   },
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "source": "model, meta = CharBiLSTM.load_model()",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-09T23:21:33.825768Z",
     "iopub.execute_input": "2025-12-09T23:21:33.826288Z",
     "iopub.status.idle": "2025-12-09T23:21:35.012994Z",
     "shell.execute_reply.started": "2025-12-09T23:21:33.826266Z",
     "shell.execute_reply": "2025-12-09T23:21:35.012416Z"
    }
   },
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "source": "meta",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-09T23:21:36.205089Z",
     "iopub.execute_input": "2025-12-09T23:21:36.205818Z",
     "iopub.status.idle": "2025-12-09T23:21:36.211331Z",
     "shell.execute_reply.started": "2025-12-09T23:21:36.205794Z",
     "shell.execute_reply": "2025-12-09T23:21:36.210480Z"
    }
   },
   "outputs": [
    {
     "execution_count": 13,
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'model_type': 'CharLSTM',\n 'best_f1': 0.934231058215857,\n 'embedding_size': 300,\n 'hidden_size': 256,\n 'num_layers': 5,\n 'dropout_rate': 0.2,\n 'learning_rate': 0.001,\n 'num_epochs': 19,\n 'max_sequence_length': 600,\n 'batch_size': 32,\n 'vocab_size': 44,\n 'output_classes': 16,\n 'model_file': 'best_model.pkl'}"
     },
     "metadata": {}
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "source": "predictor = Predictor(model, CHAR_TO_INDEX, INDEX_TO_LABEL, device=DEVICE)",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-09T23:21:48.280996Z",
     "iopub.execute_input": "2025-12-09T23:21:48.281556Z",
     "iopub.status.idle": "2025-12-09T23:21:48.291375Z",
     "shell.execute_reply.started": "2025-12-09T23:21:48.281533Z",
     "shell.execute_reply": "2025-12-09T23:21:48.290845Z"
    }
   },
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "source": "train_loader = dataset_builder.create_dataloader(data_type='train', batch_size=TRAIN_BATCH_SIZE)",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-09T11:57:57.843220Z",
     "iopub.execute_input": "2025-12-09T11:57:57.843868Z",
     "iopub.status.idle": "2025-12-09T11:58:28.727564Z",
     "shell.execute_reply.started": "2025-12-09T11:57:57.843844Z",
     "shell.execute_reply": "2025-12-09T11:58:28.726906Z"
    }
   },
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "source": "predictor.evaluate(train_loader)",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-09T11:58:28.728781Z",
     "iopub.execute_input": "2025-12-09T11:58:28.729056Z",
     "iopub.status.idle": "2025-12-09T12:02:00.396263Z",
     "shell.execute_reply.started": "2025-12-09T11:58:28.729031Z",
     "shell.execute_reply": "2025-12-09T12:02:00.395427Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "text": "Evaluating: 100%|██████████| 1678/1678 [03:31<00:00,  7.93it/s]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "8029990\nTest Accuracy: 96.151%\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\n",
     "output_type": "stream"
    },
    {
     "execution_count": 14,
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.9615052569138062"
     },
     "metadata": {}
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "source": "val_loader = dataset_builder.create_dataloader(data_type='val', batch_size=VAL_BATCH_SIZE)",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-09T12:04:48.204287Z",
     "iopub.execute_input": "2025-12-09T12:04:48.204878Z",
     "iopub.status.idle": "2025-12-09T12:04:49.691161Z",
     "shell.execute_reply.started": "2025-12-09T12:04:48.204852Z",
     "shell.execute_reply": "2025-12-09T12:04:49.690538Z"
    }
   },
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "source": "predictor.evaluate(val_loader)",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-09T12:04:51.999705Z",
     "iopub.execute_input": "2025-12-09T12:04:51.999977Z",
     "iopub.status.idle": "2025-12-09T12:05:00.110303Z",
     "shell.execute_reply.started": "2025-12-09T12:04:51.999958Z",
     "shell.execute_reply": "2025-12-09T12:05:00.109732Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "text": "Evaluating: 100%|██████████| 11/11 [00:08<00:00,  1.36it/s]\n",
     "output_type": "stream"
    },
    {
     "execution_count": 18,
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.9691978862307833"
     },
     "metadata": {}
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "source": "test_loader = dataset_builder.create_dataloader(data_type='test', batch_size=VAL_BATCH_SIZE, with_labels=False)",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-09T12:09:35.476918Z",
     "iopub.execute_input": "2025-12-09T12:09:35.477264Z",
     "iopub.status.idle": "2025-12-09T12:09:36.304155Z",
     "shell.execute_reply.started": "2025-12-09T12:09:35.477242Z",
     "shell.execute_reply": "2025-12-09T12:09:36.303550Z"
    }
   },
   "outputs": [],
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "source": "predictor.predict_dataset(test_loader);",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-09T12:10:09.366057Z",
     "iopub.execute_input": "2025-12-09T12:10:09.366804Z",
     "iopub.status.idle": "2025-12-09T12:10:16.724801Z",
     "shell.execute_reply.started": "2025-12-09T12:10:09.366778Z",
     "shell.execute_reply": "2025-12-09T12:10:16.723790Z"
    }
   },
   "outputs": [],
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "source": "import pandas as pd\ntest = pd.read_csv('/kaggle/working/submission.csv')\ntest",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-09T12:11:16.286684Z",
     "iopub.execute_input": "2025-12-09T12:11:16.287280Z",
     "iopub.status.idle": "2025-12-09T12:11:16.346998Z",
     "shell.execute_reply.started": "2025-12-09T12:11:16.287255Z",
     "shell.execute_reply": "2025-12-09T12:11:16.346296Z"
    }
   },
   "outputs": [
    {
     "execution_count": 33,
     "output_type": "execute_result",
     "data": {
      "text/plain": "            ID  label\n0            0      4\n1            1     14\n2            2     14\n3            3      6\n4            4      4\n...        ...    ...\n237235  237235      6\n237236  237236      4\n237237  237237      0\n237238  237238      6\n237239  237239      3\n\n[237240 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>14</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>14</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>237235</th>\n      <td>237235</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>237236</th>\n      <td>237236</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>237237</th>\n      <td>237237</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>237238</th>\n      <td>237238</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>237239</th>\n      <td>237239</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n<p>237240 rows × 2 columns</p>\n</div>"
     },
     "metadata": {}
    }
   ],
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "source": "import pandas as pd\ntest = pd.read_csv('/kaggle/input/contest-data/cufe-cmp-mainstream-nlp-fall-2025/test_no_diacritics.csv')\ntest",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-09T12:13:03.250955Z",
     "iopub.execute_input": "2025-12-09T12:13:03.251571Z",
     "iopub.status.idle": "2025-12-09T12:13:03.360836Z",
     "shell.execute_reply.started": "2025-12-09T12:13:03.251546Z",
     "shell.execute_reply": "2025-12-09T12:13:03.360170Z"
    }
   },
   "outputs": [
    {
     "execution_count": 35,
     "output_type": "execute_result",
     "data": {
      "text/plain": "            id  line_number letter  case_ending\n0            0            0      ف        False\n1            1            0      ي         True\n2            2            0      ا        False\n3            3            0      ل        False\n4            4            0      م        False\n...        ...          ...    ...          ...\n237235  237235         2468      ب        False\n237236  237236         2468      ن         True\n237237  237237         2468      ش        False\n237238  237238         2468      ي        False\n237239  237239         2468      ء         True\n\n[237240 rows x 4 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>line_number</th>\n      <th>letter</th>\n      <th>case_ending</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>ف</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>0</td>\n      <td>ي</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>0</td>\n      <td>ا</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>0</td>\n      <td>ل</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>0</td>\n      <td>م</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>237235</th>\n      <td>237235</td>\n      <td>2468</td>\n      <td>ب</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>237236</th>\n      <td>237236</td>\n      <td>2468</td>\n      <td>ن</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>237237</th>\n      <td>237237</td>\n      <td>2468</td>\n      <td>ش</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>237238</th>\n      <td>237238</td>\n      <td>2468</td>\n      <td>ي</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>237239</th>\n      <td>237239</td>\n      <td>2468</td>\n      <td>ء</td>\n      <td>True</td>\n    </tr>\n  </tbody>\n</table>\n<p>237240 rows × 4 columns</p>\n</div>"
     },
     "metadata": {}
    }
   ],
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "source": "gold = pd.read_csv('submission.csv')\ngold",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-09T12:13:18.454023Z",
     "iopub.execute_input": "2025-12-09T12:13:18.454631Z",
     "iopub.status.idle": "2025-12-09T12:13:18.491934Z",
     "shell.execute_reply.started": "2025-12-09T12:13:18.454607Z",
     "shell.execute_reply": "2025-12-09T12:13:18.491279Z"
    }
   },
   "outputs": [
    {
     "execution_count": 36,
     "output_type": "execute_result",
     "data": {
      "text/plain": "            ID  label\n0            0      4\n1            1     14\n2            2     14\n3            3      6\n4            4      4\n...        ...    ...\n237235  237235      6\n237236  237236      4\n237237  237237      0\n237238  237238      6\n237239  237239      3\n\n[237240 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>14</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>14</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>237235</th>\n      <td>237235</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>237236</th>\n      <td>237236</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>237237</th>\n      <td>237237</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>237238</th>\n      <td>237238</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>237239</th>\n      <td>237239</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n<p>237240 rows × 2 columns</p>\n</div>"
     },
     "metadata": {}
    }
   ],
   "execution_count": 36
  },
  {
   "cell_type": "code",
   "source": "gold[test.case_ending]",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-09T12:13:24.770719Z",
     "iopub.execute_input": "2025-12-09T12:13:24.771514Z",
     "iopub.status.idle": "2025-12-09T12:13:24.781918Z",
     "shell.execute_reply.started": "2025-12-09T12:13:24.771489Z",
     "shell.execute_reply": "2025-12-09T12:13:24.781128Z"
    }
   },
   "outputs": [
    {
     "execution_count": 37,
     "output_type": "execute_result",
     "data": {
      "text/plain": "            ID  label\n1            1     14\n7            7      4\n12          12     12\n18          18      4\n20          20      6\n...        ...    ...\n237224  237224     14\n237228  237228      2\n237231  237231     14\n237236  237236      4\n237239  237239      3\n\n[56736 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>14</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>7</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>12</td>\n      <td>12</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>18</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>20</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>237224</th>\n      <td>237224</td>\n      <td>14</td>\n    </tr>\n    <tr>\n      <th>237228</th>\n      <td>237228</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>237231</th>\n      <td>237231</td>\n      <td>14</td>\n    </tr>\n    <tr>\n      <th>237236</th>\n      <td>237236</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>237239</th>\n      <td>237239</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n<p>56736 rows × 2 columns</p>\n</div>"
     },
     "metadata": {}
    }
   ],
   "execution_count": 37
  },
  {
   "cell_type": "code",
   "source": "gold[test.case_ending].to_csv(\"saved_data.csv\", index=False, encoding=\"utf-8\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-09T12:13:32.967645Z",
     "iopub.execute_input": "2025-12-09T12:13:32.968205Z",
     "iopub.status.idle": "2025-12-09T12:13:33.016662Z",
     "shell.execute_reply.started": "2025-12-09T12:13:32.968179Z",
     "shell.execute_reply": "2025-12-09T12:13:33.015880Z"
    }
   },
   "outputs": [],
   "execution_count": 38
  },
  {
   "cell_type": "code",
   "source": [
    "# from huggingface_hub import hf_hub_download\n",
    "# from huggingface_hub import login\n",
    "# repo_id = \"OmarHashem80/LSTM98\"\n",
    "# model_file = \"best_model.pkl\"\n",
    "# meta_file = \"best_model_meta.json\"\n",
    "\n",
    "# # download from HF Hub\n",
    "# local_model_path = hf_hub_download(repo_id=repo_id, filename=model_file)\n",
    "# local_meta_path = hf_hub_download(repo_id=repo_id, filename=meta_file)\n",
    "\n",
    "# print(\"Downloaded model:\", local_model_path)\n",
    "# import shutil\n",
    "\n",
    "# destination_path = '/kaggle/working/best_model.pkl'\n",
    "\n",
    "# shutil.copy(local_model_path, destination_path)\n",
    "# print(f'Model saved to {destination_path}')\n",
    "\n",
    "# destination_path = '/kaggle/working/best_model_meta.json'\n",
    "\n",
    "# shutil.copy(local_meta_path, destination_path)\n",
    "# print(f'Model saved to {destination_path}')"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-09T22:39:38.859971Z",
     "iopub.execute_input": "2025-12-09T22:39:38.860699Z",
     "iopub.status.idle": "2025-12-09T22:39:40.745664Z",
     "shell.execute_reply.started": "2025-12-09T22:39:38.860677Z",
     "shell.execute_reply": "2025-12-09T22:39:40.744058Z"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "best_model_meta.json:   0%|          | 0.00/339 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9740aa39fa324a9a9cdac3af2f1b1b11"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "Downloaded model: /root/.cache/huggingface/hub/models--OmarHashem80--LSTM98/snapshots/064a2a433e8f9d0d2a72b0f2185af280380d6fe8/best_model.pkl\nModel saved to /kaggle/working/best_model.pkl\nModel saved to /kaggle/working/best_model_meta.json\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "source": "max_len = 600",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-09T22:44:39.199565Z",
     "iopub.execute_input": "2025-12-09T22:44:39.200180Z",
     "iopub.status.idle": "2025-12-09T22:44:39.203804Z",
     "shell.execute_reply.started": "2025-12-09T22:44:39.200156Z",
     "shell.execute_reply": "2025-12-09T22:44:39.202974Z"
    }
   },
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "source": "model_file = \"best_model.pkl\"\nmeta_file = \"best_model_meta.json\"\nmodel, _ = CharBiLSTM.load_model(model_file, meta_file)",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-09T23:20:17.009951Z",
     "iopub.execute_input": "2025-12-09T23:20:17.010654Z",
     "iopub.status.idle": "2025-12-09T23:20:19.109084Z",
     "shell.execute_reply.started": "2025-12-09T23:20:17.010612Z",
     "shell.execute_reply": "2025-12-09T23:20:19.108496Z"
    }
   },
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "source": "def test_last_char_text(model, data_loader, max_len=600, batch_size=256,\n                        char_to_index=CHAR_TO_INDEX, index_to_label=INDEX_TO_LABEL, labels=LABELS, index_to_char=INDEX_TO_CHAR):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(device)\n\n    model.eval()\n\n    total_last_char = 0\n    correct_last_char = 0\n    words_text = []\n\n    with torch.inference_mode():\n        for batch_sequences, batch_labels in tqdm(data_loader, desc=\"Last Char Test\"):\n            outputs = model(batch_sequences)  # batch_size * seq_length * output_size\n            predicted_labels = outputs.argmax(dim=2)\n\n            batch_size_seq = batch_sequences.shape[0]\n\n            for i in range(batch_size_seq):\n                seq = batch_sequences[i]\n                true_labels = batch_labels[i]\n                pred_labels = predicted_labels[i]\n\n                word = ''\n                last_char_true = None\n                last_char_pred = None\n\n                for idx, c in enumerate(seq):\n                    if c in [0, 2, 8, 15, 16, 26, 40, 43]:  # padding/unwanted chars\n                        if word:\n                            last_char_true_val = last_char_true if last_char_true is not None else 0\n                            last_char_pred_val = last_char_pred if last_char_pred is not None else 0\n                            words_text.append(f\"{word}:{last_char_true_val}->{last_char_pred_val}\")\n                            if last_char_true_val == last_char_pred_val:\n                                correct_last_char += 1\n                            total_last_char += 1\n\n                            word = ''\n                            last_char_true = None\n                            last_char_pred = None\n                        continue\n\n                    word += index_to_char[int(c)]\n                    last_char_true = index_to_label[int(true_labels[idx])]\n                    last_char_pred = index_to_label[int(pred_labels[idx])]\n\n                # catch last word in sequence\n                if word:\n                    last_char_true_val = last_char_true if last_char_true is not None else 0\n                    last_char_pred_val = last_char_pred if last_char_pred is not None else 0\n                    words_text.append(f\"{word}:{last_char_true_val}->{last_char_pred_val}\")\n                    if last_char_true_val == last_char_pred_val:\n                        correct_last_char += 1\n                    total_last_char += 1\n\n    accuracy = correct_last_char / total_last_char if total_last_char > 0 else 0\n    print(f\"Last Character Accuracy: {accuracy*100:.3f}%\")\n\n    return accuracy\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-09T23:29:48.284207Z",
     "iopub.execute_input": "2025-12-09T23:29:48.284497Z",
     "iopub.status.idle": "2025-12-09T23:29:48.293022Z",
     "shell.execute_reply.started": "2025-12-09T23:29:48.284477Z",
     "shell.execute_reply": "2025-12-09T23:29:48.292322Z"
    }
   },
   "outputs": [],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "source": "val_loader = dataset_builder.create_dataloader(data_type='val', batch_size=VAL_BATCH_SIZE)",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-09T23:25:17.111791Z",
     "iopub.execute_input": "2025-12-09T23:25:17.112488Z",
     "iopub.status.idle": "2025-12-09T23:25:18.495282Z",
     "shell.execute_reply.started": "2025-12-09T23:25:17.112462Z",
     "shell.execute_reply": "2025-12-09T23:25:18.494687Z"
    }
   },
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "source": "test_last_char_text(model, val_loader)",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-09T23:21:58.114250Z",
     "iopub.execute_input": "2025-12-09T23:21:58.114520Z",
     "iopub.status.idle": "2025-12-09T23:24:11.149458Z",
     "shell.execute_reply.started": "2025-12-09T23:21:58.114502Z",
     "shell.execute_reply": "2025-12-09T23:24:11.148835Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "cuda\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "Last Char Test: 100%|██████████| 11/11 [02:11<00:00, 11.96s/it]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Last Character Accuracy: 96.553%\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\n",
     "output_type": "stream"
    },
    {
     "execution_count": 16,
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.9655344021702007"
     },
     "metadata": {}
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "source": "train_loader = dataset_builder.create_dataloader(data_type='train', batch_size=VAL_BATCH_SIZE)",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-09T23:31:18.952371Z",
     "iopub.execute_input": "2025-12-09T23:31:18.953119Z",
     "iopub.status.idle": "2025-12-09T23:31:48.002336Z",
     "shell.execute_reply.started": "2025-12-09T23:31:18.953085Z",
     "shell.execute_reply": "2025-12-09T23:31:48.001680Z"
    }
   },
   "outputs": [],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "source": "test_last_char_text(model, data_loader=train_loader)",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-09T23:31:48.003462Z",
     "iopub.execute_input": "2025-12-09T23:31:48.003709Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "cuda\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "Last Char Test:  10%|▉         | 20/210 [04:11<39:42, 12.54s/it]",
     "output_type": "stream"
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  }
 ]
}
