{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-12-09T02:03:37.945262Z",
     "iopub.status.busy": "2025-12-09T02:03:37.944608Z",
     "iopub.status.idle": "2025-12-09T02:03:37.956777Z",
     "shell.execute_reply": "2025-12-09T02:03:37.956077Z",
     "shell.execute_reply.started": "2025-12-09T02:03:37.945242Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# import textwrap\n",
    "# import re\n",
    "# import torch\n",
    "# from torch.utils.data import TensorDataset, DataLoader\n",
    "# import torch.nn as nn\n",
    "# from torch.nn import BatchNorm1d\n",
    "# from torch.optim.lr_scheduler import StepLR\n",
    "# import warnings\n",
    "# from tqdm import tqdm  \n",
    "\n",
    "# # model_path = 'best_model.pkl'\n",
    "# model_path = '/kaggle/input/rnn97/pytorch/default/1/BiLSTM_Loss=0.0325396_Accuracy=97.768%_embedding_size=300hidden_size=256lr=0.001num_layers=5num_epochs=19max_len=600batch_size=32.pkl'\n",
    "\n",
    "# char_to_index = {'د': 1, '؟': 2, 'آ': 3, 'إ': 4, 'ؤ': 5, 'ط': 6, 'م': 7, '،': 8, 'ة': 9, 'ت': 10, 'ر': 11, 'ئ': 12, 'ا': 13, 'ض': 14, '!': 15, ' ': 16, 'ك': 17, 'غ': 18, 'س': 19, 'ص': 20, 'أ': 21, 'ل': 22, 'ف': 23, 'ظ': 24, 'ج': 25, '؛': 26, 'ن': 27, 'ع': 28, 'ب': 29, 'ث': 30, 'ه': 31, 'خ': 32, 'ى': 33, 'ء': 34, 'ز': 35, 'ق': 36, 'ي': 37, 'ش': 38, 'ح': 39, ':': 40, 'ذ': 41, 'و': 42, '.': 43}\n",
    "# index_to_char = {1: 'د', 2: '؟', 3: 'آ', 4: 'إ', 5: 'ؤ', 6: 'ط', 7: 'م', 8: '،', 9: 'ة', 10: 'ت', 11: 'ر', 12: 'ئ', 13: 'ا', 14: 'ض', 15: '!', 16: ' ', 17: 'ك', 18: 'غ', 19: 'س', 20: 'ص', 21: 'أ', 22: 'ل', 23: 'ف', 24: 'ظ', 25: 'ج', 26: '؛', 27: 'ن', 28: 'ع', 29: 'ب', 30: 'ث', 31: 'ه', 32: 'خ', 33: 'ى', 34: 'ء', 35: 'ز', 36: 'ق', 37: 'ي', 38: 'ش', 39: 'ح', 40: ':', 41: 'ذ', 42: 'و', 43: '.'}\n",
    "\n",
    "# labels = {\n",
    "#     # fath\n",
    "#     1614: 0,\n",
    "#     # tanween bel fath\n",
    "#     1611: 1,\n",
    "#     # damm\n",
    "#     1615: 2,\n",
    "#     # tanween bel damm\n",
    "#     1612: 3,\n",
    "#     # kasr\n",
    "#     1616: 4,\n",
    "#     # tanween bel kasr\n",
    "#     1613: 5,\n",
    "#     # sukun\n",
    "#     1618: 6,\n",
    "#     # shadd\n",
    "#     1617: 7,\n",
    "#     # shadd and fath\n",
    "#     (1617, 1614): 8,\n",
    "#     # shadd and tanween bel fath\n",
    "#     (1617, 1611): 9,\n",
    "#     # shadd and damm\n",
    "#     (1617, 1615): 10,\n",
    "#     # shadd and tanween bel damm\n",
    "#     (1617, 1612): 11,\n",
    "#     # shadd and kasr\n",
    "#     (1617, 1616): 12,\n",
    "#     # shadd and tanween bel kasr\n",
    "#     (1617, 1613): 13,\n",
    "#     # no diacritic\n",
    "#     0: 14,\n",
    "#     # padded\n",
    "#     15: 15\n",
    "# }\n",
    "\n",
    "# indicies_to_labels = {\n",
    "#     # fath\n",
    "#     0: 1614,\n",
    "#     # tanween bel fath\n",
    "#     1: 1611,\n",
    "#     # damm\n",
    "#     2: 1615,\n",
    "#     # tanween bel damm\n",
    "#     3: 1612,\n",
    "#     # kasr\n",
    "#     4: 1616,\n",
    "#     # tanween bel kasr\n",
    "#     5: 1613,\n",
    "#     # sukun\n",
    "#     6: 1618,\n",
    "#     # shadd\n",
    "#     7: 1617,\n",
    "#     # shadd and fath\n",
    "#     8: (1617, 1614),\n",
    "#     # shadd and tanween bel fath\n",
    "#     9: (1617, 1611),\n",
    "#     # shadd and damm\n",
    "#     10: (1617, 1615),\n",
    "#     # shadd and tanween bel damm\n",
    "#     11: (1617, 1612),\n",
    "#     # shadd and kasr\n",
    "#     12: (1617, 1616),\n",
    "#     # shadd and tanween bel kasr\n",
    "#     13: (1617, 1613),\n",
    "#     # no diacritic\n",
    "#     14: 0,\n",
    "#     # padded\n",
    "#     15: 15\n",
    "# }\n",
    "\n",
    "# max_len = 600\n",
    "\n",
    "# training_batch_size = 32\n",
    "# validation_batch_size = 256\n",
    "\n",
    "# dataset_path = '/kaggle/input/nlp-project-data/data'\n",
    "\n",
    "# def replace_pattern(text,pattern,replace = ''):\n",
    "#     cleaned_text = pattern.sub(replace, text)\n",
    "#     return cleaned_text\n",
    "\n",
    "# def clean(lines):\n",
    "#     for i in range(len(lines)):\n",
    "#         reg = r'\\(\\s*(\\d+)\\s*\\)|\\(\\s*(\\d+)\\s*\\/\\s*(\\d+)\\s*\\)|\\d+'\n",
    "#         lines[i] = replace_pattern(lines[i], re.compile(reg))\n",
    "#         reg_brackets = r'[\\[\\{\\(\\]\\}\\)]'\n",
    "#         lines[i] = re.compile(reg_brackets).sub('', lines[i])\n",
    "#         reg = r'[/\\/\\\\\\-]'\n",
    "#         lines[i] = replace_pattern(lines[i], re.compile(reg))\n",
    "#         reg = r'[,»–\\';«*\\u200f\"\\\\~`]'\n",
    "#         lines[i] = replace_pattern(lines[i], re.compile(reg))\n",
    "#         reg = r'\\s+'\n",
    "#         lines[i] = replace_pattern(lines[i], re.compile(reg), ' ')\n",
    "#     return lines\n",
    "\n",
    "# def remove_diactrics(lines):\n",
    "#     for i in range(len(lines)):\n",
    "#         reg = r'[\\u064B-\\u065F\\u0670\\uFE70-\\uFE7F]'\n",
    "#         lines[i] = replace_pattern(lines[i], re.compile(reg))\n",
    "#     return lines\n",
    "\n",
    "# def preprocess(lines, data_type, dataset_path = '', with_labels = False):\n",
    "#     lines = clean(lines)\n",
    "#     if len(lines) == 0:\n",
    "#         return lines\n",
    "#     if with_labels:\n",
    "#         with open(f'{data_type}_with_labels.txt', 'a+',encoding='utf-8') as f:\n",
    "#             f.write('\\n'.join(lines))\n",
    "#             f.write('\\n')\n",
    "#     lines = remove_diactrics(lines)\n",
    "#     with open(f'{data_type}_without_labels.txt', 'a+',encoding='utf-8') as f:\n",
    "#         f.write('\\n'.join(lines))\n",
    "#         f.write('\\n')\n",
    "#     return lines\n",
    "\n",
    "# def preprocess_data(data_type, limit = None, dataset_path = '.', with_labels = True):\n",
    "#     # with open(f'{dataset_path}cleaned_{data_type}_data_with_diacritics.txt', 'w',encoding='utf-8') as f:\n",
    "#     #     pass\n",
    "#     # with open(f'{dataset_path}cleaned_{data_type}_data_without_diacritics.txt', 'w',encoding='utf-8') as f:\n",
    "#     #     pass\n",
    "#     sentences = []\n",
    "#     # with open(f'{dataset_path}{data_type}.txt', 'r',encoding='utf-8') as f:\n",
    "#     with open(f'/kaggle/input/nlp-project-data/data/train.txt', 'r',encoding='utf-8') as f:\n",
    "#         lines = f.readlines()\n",
    "#         lines = [line.strip() for line in lines]\n",
    "#         if limit == None:\n",
    "#             limit = len(lines)\n",
    "#         lines = lines[:limit]\n",
    "#         sentences = preprocess(lines, data_type, dataset_path, with_labels=with_labels)\n",
    "\n",
    "#     return sentences\n",
    "\n",
    "# def count_spaces(input_string):\n",
    "#     space_count = len(re.findall(r'\\s', input_string))\n",
    "#     return space_count\n",
    "\n",
    "# def tokenize_data(data_type='train', dataset_path='.', max_len=200, with_labels=True):\n",
    "#     data = []\n",
    "#     spaces = []\n",
    "#     dataset_path = '/kaggle/working/'\n",
    "#     with open(f'{dataset_path}{data_type}_without_labels.txt', 'r', encoding='utf-8') as file:\n",
    "#         data_lines = file.readlines()\n",
    "#         for i in range(len(data_lines)):\n",
    "#             data_lines[i] = re.compile(r'[\\n\\r\\t]').sub('', data_lines[i])\n",
    "#             data_lines[i] = re.compile(r'\\s+').sub(' ', data_lines[i])\n",
    "#             data_lines[i] = data_lines[i].strip()\n",
    "#             sentences = textwrap.wrap(data_lines[i], max_len)\n",
    "#             for sentence in sentences:\n",
    "#                 data.append(sentence)\n",
    "#                 spaces.append(count_spaces(sentence))   \n",
    "#     data_with_diacritics = []\n",
    "#     if with_labels:\n",
    "#         spaces_index = 0\n",
    "#         with open(f'{dataset_path}cleaned_{data_type}_data_with_diacritics.txt', 'r', encoding='utf-8') as file:\n",
    "#             data_with_diacritics_lines = file.readlines()\n",
    "#             for i in range(len(data_with_diacritics_lines)):\n",
    "#                 data_with_diacritics_lines[i] = re.compile(r'[\\n\\r\\t]').sub('', data_with_diacritics_lines[i])\n",
    "#                 data_with_diacritics_lines[i] = re.compile(r'\\s+').sub(' ', data_with_diacritics_lines[i])\n",
    "#                 data_with_diacritics_lines[i] = data_with_diacritics_lines[i].strip()\n",
    "\n",
    "#                 remaining = data_with_diacritics_lines[i]\n",
    "#                 remaining_length = len(remaining)\n",
    "#                 while(remaining_length > 0):\n",
    "#                     spaces_to_include = spaces[spaces_index]\n",
    "#                     spaces_index += 1\n",
    "#                     words = remaining.split()\n",
    "#                     if len(words) <= spaces_to_include + 1:\n",
    "#                         data_with_diacritics.append(remaining.strip())\n",
    "#                         remaining_length = 0\n",
    "#                         break\n",
    "#                     else:\n",
    "#                         sentence = ' '.join(words[:spaces_to_include + 1])\n",
    "#                         data_with_diacritics.append(sentence.strip())\n",
    "#                         remaining = ' '.join(words[spaces_to_include + 1:]).strip()\n",
    "#                         remaining_length = len(remaining)\n",
    "                        \n",
    "#     return data, data_with_diacritics\n",
    "\n",
    "# def convert2idx(data=[], char_to_index={}, max_len=200, device='cpu'):\n",
    "#     data_sequences = [[char_to_index[char] for char in sequence] for sequence in data]\n",
    "#     data_sequences = [sequence + [0] * (max_len - len(sequence)) for sequence in data_sequences]\n",
    "#     data_sequences = torch.tensor(data_sequences).to(device)\n",
    "#     return data_sequences\n",
    "\n",
    "# def label_data(data_with_diacritics=[], labels={}, max_len=200, device='cpu'):\n",
    "#     data_labels = []\n",
    "#     size = len(data_with_diacritics)\n",
    "#     for sentence_index in range(size):\n",
    "#         sentence_labels = []\n",
    "#         sentence_size = len(data_with_diacritics[sentence_index])\n",
    "#         index = 0\n",
    "#         while index < sentence_size:\n",
    "#             if ord(data_with_diacritics[sentence_index][index]) not in labels:\n",
    "#                 char_sequence = char_to_index[data_with_diacritics[sentence_index][index]]\n",
    "#                 if char_sequence == 2 or char_sequence == 8 or char_sequence == 15 or char_sequence == 16 or char_sequence == 26 or char_sequence == 40 or char_sequence == 43:\n",
    "#                     sentence_labels.append(14)\n",
    "#                     index += 1\n",
    "#                     continue\n",
    "#                 if (index + 1) < sentence_size and ord(data_with_diacritics[sentence_index][index + 1]) in labels:\n",
    "#                     if ord(data_with_diacritics[sentence_index][index + 1]) == 1617:\n",
    "#                         if (index + 2) < sentence_size and ord(data_with_diacritics[sentence_index][index + 2]) in labels:\n",
    "#                             sentence_labels.append(labels[(1617, ord(data_with_diacritics[sentence_index][index + 2]))])\n",
    "#                             index += 3  \n",
    "#                             continue\n",
    "#                         else:\n",
    "#                             sentence_labels.append(labels[1617])\n",
    "#                             index += 2\n",
    "#                             continue\n",
    "#                     sentence_labels.append(labels[ord(data_with_diacritics[sentence_index][index + 1])])\n",
    "#                     index += 2 \n",
    "#                     continue\n",
    "#                 else:\n",
    "#                     sentence_labels.append(14)\n",
    "#             index += 1\n",
    "\n",
    "#         data_labels.append(sentence_labels)\n",
    "#     data_labels = [sequence + [15] * (max_len - len(sequence)) for sequence in data_labels]\n",
    "#     data_labels = torch.tensor(data_labels).to(device)\n",
    "#     return data_labels\n",
    "\n",
    "# def get_dataloader(data_type='train', max_len=200, batch_size=256, dataset_path='', char_to_index={}, labels={}, device='cpu', with_labels=True):\n",
    "#     preprocess_data(data_type=data_type, dataset_path=dataset_path, with_labels=with_labels)\n",
    "#     data, data_with_diacritics = tokenize_data(data_type=data_type, dataset_path=dataset_path, max_len=max_len, with_labels=with_labels)\n",
    "#     data_sequences = convert2idx(data=data, char_to_index=char_to_index, max_len=max_len, device=device)\n",
    "#     if with_labels:\n",
    "#         data_labels = label_data(data_with_diacritics=data_with_diacritics, labels=labels, max_len=max_len, device=device)\n",
    "#     else:\n",
    "#         data_labels = torch.tensor([[15] * max_len] * len(data_sequences)).to(device)\n",
    "#     print(f'{data_type} data shape: ', len(data))\n",
    "#     print(f'{data_type} data sequences shape: ', data_sequences.shape)\n",
    "#     dataset = TensorDataset(data_sequences, data_labels)\n",
    "#     dataloader = DataLoader(dataset, batch_size=batch_size)\n",
    "#     return dataloader\n",
    "\n",
    "# class CharLSTM(nn.Module):\n",
    "#     def __init__(self, vocab_size, embedding_size, hidden_size, output_size, dropout_rate, num_layers=1):\n",
    "#         super(CharLSTM, self).__init__()\n",
    "#         self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "#         self.lstm = nn.LSTM(input_size=embedding_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True, bidirectional=True, dropout=dropout_rate)\n",
    "#         self.batchnorm = BatchNorm1d(max_len)\n",
    "#         self.output = nn.Linear(hidden_size * 2, output_size)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         embedded = self.embedding(x)\n",
    "#         lstm_out, _ = self.lstm(embedded) \n",
    "#         lstm_out = self.batchnorm(lstm_out) \n",
    "#         output = self.output(lstm_out)\n",
    "#         return output\n",
    "    \n",
    "# def train():\n",
    "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"); print(device)\n",
    "#     print(\"train preprocessing\")\n",
    "#     training_dataloader = get_dataloader(data_type='train', max_len=max_len, batch_size=training_batch_size, dataset_path=dataset_path, char_to_index=char_to_index, labels=labels, device=device, with_labels=True)\n",
    "#     print(\"val preprocessing\")\n",
    "#     validation_dataloader = get_dataloader(data_type='val', max_len=max_len, batch_size=validation_batch_size, dataset_path=dataset_path, char_to_index=char_to_index, labels=labels, device=device, with_labels=True)\n",
    "\n",
    "#     num_layers = 5\n",
    "#     vocab_size = len(char_to_index) + 1 \n",
    "#     embedding_size = 300\n",
    "#     output_size = len(labels)\n",
    "#     hidden_size = 256\n",
    "#     lr=0.001\n",
    "#     num_epochs = 19\n",
    "#     dropout_rate = 0.2\n",
    "#     lr_step_size = 5\n",
    "#     lr_gamma = 0.1\n",
    "\n",
    "#     model = CharLSTM(vocab_size, embedding_size, hidden_size, output_size, dropout_rate, num_layers).to(device)\n",
    "#     criterion = nn.CrossEntropyLoss(ignore_index=15)\n",
    "#     optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.999), eps=1e-08)\n",
    "#     scheduler = StepLR(optimizer, step_size=lr_step_size, gamma=lr_gamma)\n",
    "\n",
    "#     for epoch in range(num_epochs):\n",
    "#         model.train()\n",
    "#         epoch_loss = 0\n",
    "#         # tqdm for training batches\n",
    "#         for batch_sequences, batch_labels in tqdm(training_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=True):\n",
    "#             optimizer.zero_grad()\n",
    "#             outputs = model(batch_sequences)\n",
    "#             flat_outputs = outputs.view(-1, outputs.shape[-1])\n",
    "#             flat_labels = batch_labels.view(-1)\n",
    "#             mask = (flat_labels != 15)\n",
    "#             loss = criterion(flat_outputs[mask], flat_labels[mask])\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             epoch_loss += loss.item()\n",
    "    \n",
    "#         last_loss = epoch_loss / len(training_dataloader)\n",
    "#         model.eval()\n",
    "#         correct_predictions = 0\n",
    "#         total_predictions = 0\n",
    "#         with torch.inference_mode():\n",
    "            \n",
    "#             for validation_batch_sequences, validation_batch_labels in tqdm(validation_dataloader, desc=\"Validation\", leave=True):\n",
    "#                 outputs = model(validation_batch_sequences) \n",
    "#                 predicted_labels = outputs.argmax(dim=2) \n",
    "#                 mask = (validation_batch_labels != 15) & (validation_batch_sequences != 2) & (validation_batch_sequences != 8) & (validation_batch_sequences != 15) & (validation_batch_sequences != 16) & (validation_batch_sequences != 26) & (validation_batch_sequences != 40) & (validation_batch_sequences != 43)\n",
    "#                 correct_predictions += ((predicted_labels == validation_batch_labels) & mask).sum().item()\n",
    "#                 total_predictions += mask.sum().item()\n",
    "#         scheduler.step()\n",
    "#         accuracy = correct_predictions / total_predictions\n",
    "#         print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {last_loss * 1:.7f}, Accuracy: {accuracy * 100:.3f}%')\n",
    "#     file_path = f\"BiLSTM_Loss={last_loss * 1:.7f}_Accuracy={accuracy * 100:.3f}%_embedding_size={embedding_size}hidden_size={hidden_size}lr={lr}num_layers={num_layers}num_epochs={num_epochs}max_len={max_len}batch_size={training_batch_size}.pkl\"\n",
    "#     with open(file_path, 'wb') as file:\n",
    "#         pickle.dump(model, file)\n",
    "\n",
    "# def load_model(model_path):\n",
    "#     with open(model_path, \"rb\") as file:\n",
    "#         model = pickle.load(file)\n",
    "#     return model\n",
    "\n",
    "# def predict_test(model):\n",
    "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"); print(device)\n",
    "#     test_dataloader = get_dataloader(data_type='test', max_len=max_len, batch_size=validation_batch_size, dataset_path=dataset_path, char_to_index=char_to_index, labels=labels, device=device, with_labels=False) \n",
    "#     with open('submission.csv', 'w', encoding='utf-8') as file:\n",
    "#         file.write('ID,label')\n",
    "#         predicted_labels = []\n",
    "#         model.eval()\n",
    "#         with torch.inference_mode():\n",
    "#             for test_batch_sequences, _ in test_dataloader:\n",
    "#                 outputs = model(test_batch_sequences) \n",
    "#                 batch_predicted_labels = outputs.argmax(dim=2)  \n",
    "#                 mask = (test_batch_sequences != 0) & (test_batch_sequences != 2) & (test_batch_sequences != 8) & (test_batch_sequences != 15) & (test_batch_sequences != 16) & (test_batch_sequences != 26) & (test_batch_sequences != 40) & (test_batch_sequences != 43)\n",
    "#                 batch_predicted_labels = batch_predicted_labels[mask]\n",
    "#                 predicted_labels.extend(batch_predicted_labels.tolist())\n",
    "#             print('predicted_labels length: ', len(predicted_labels))\n",
    "#             for i in range(len(predicted_labels)):\n",
    "#                 file.write(f'\\n{i},{predicted_labels[i]}')\n",
    "            \n",
    "# def predict_single_sentence(model, original_sentence='', max_len=200, char_to_index={}, indicies_to_labels={}, batch_size=256):\n",
    "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") #; print(device)\n",
    "#     preprocessed_sentence = original_sentence.strip()\n",
    "#     preprocessed_sentence = clean([preprocessed_sentence])[0]\n",
    "#     preprocessed_sentence = re.compile(r'[\\n\\r\\t]').sub('', preprocessed_sentence)\n",
    "#     preprocessed_sentence = re.compile(r'\\s+').sub(' ', preprocessed_sentence)\n",
    "#     preprocessed_sentence = preprocessed_sentence.strip()\n",
    "#     tokenized_sentences = []\n",
    "#     dot_splitted_list = preprocessed_sentence.split('.')\n",
    "#     if dot_splitted_list[-1] == '':\n",
    "#         dot_splitted_list = dot_splitted_list[:-1]\n",
    "#     for dot_splitted in dot_splitted_list:\n",
    "#         dot_splitted = dot_splitted.strip()\n",
    "#         sentences = textwrap.wrap(dot_splitted, max_len)\n",
    "#         for sentence in sentences:\n",
    "#             tokenized_sentences.append(sentence)\n",
    "#     sentence_sequences = convert2idx(data=tokenized_sentences, char_to_index=char_to_index, max_len=max_len, device=device)\n",
    "#     dataset = TensorDataset(sentence_sequences, sentence_sequences)\n",
    "#     dataloader = DataLoader(dataset, batch_size=batch_size)\n",
    "#     predicted_labels = []\n",
    "#     model.eval()\n",
    "#     with torch.inference_mode():\n",
    "#         for batch_sequences, batch_labels in dataloader:\n",
    "#             outputs = model(batch_sequences)\n",
    "#             batch_predicted_labels = outputs.argmax(dim=2)\n",
    "#             mask = (batch_labels != 15) & (batch_sequences != 2) & (batch_sequences != 8) & (batch_sequences != 15) & (batch_sequences != 16) & (batch_sequences != 26) & (batch_sequences != 40) & (batch_sequences != 43)\n",
    "#             batch_predicted_labels = batch_predicted_labels[mask]\n",
    "#             predicted_labels.extend(batch_predicted_labels.tolist())\n",
    "        \n",
    "#     predicted_sentence = \"\"\n",
    "#     predicted_char_index = 0\n",
    "#     for char in original_sentence:\n",
    "#         predicted_sentence += char\n",
    "#         if char not in char_to_index:\n",
    "#             continue\n",
    "#         elif char_to_index[char] == 2 or char_to_index[char] == 8 or char_to_index[char] == 15 or char_to_index[char] == 16 or char_to_index[char] == 26 or char_to_index[char] == 40 or char_to_index[char] == 43:\n",
    "#             continue\n",
    "#         predicted_class = indicies_to_labels[predicted_labels[predicted_char_index]]\n",
    "#         if type(predicted_class) is tuple:\n",
    "#             predicted_sentence += chr(predicted_class[0]) + chr(predicted_class[1])\n",
    "#             predicted_char_index += 1\n",
    "#         elif predicted_class == 0:\n",
    "#             predicted_char_index += 1\n",
    "#         else:\n",
    "#             predicted_sentence += chr(predicted_class)\n",
    "#             predicted_char_index += 1\n",
    "                \n",
    "#     return predicted_sentence\n",
    "            \n",
    "\n",
    "# # main\n",
    "# # if __name__ == \"__main__\":\n",
    "#     # warnings.filterwarnings('ignore')\n",
    "#     # train()\n",
    "#     # model = load_model(model_path)\n",
    "#     # predict_test(model)\n",
    "#     # test_sentence = \"ليس للوكيل بالقبض أن يبرأ المدين أو يهب الدين له أو يأخذ رهنا من المدين في مقابل الدين أو يقبل إحالته على شخص آخر لكن له أن يأخذ كفيلا لكن ليس له أن يأخذ كفيلا بشرط براءة الأصيل انظر المادة ( 648 ) ( الأنقروي ، الطحطاوي وصرة الفتاوى ، البحر ) .\"\n",
    "#     # print(len(test_sentence))\n",
    "#     # print(test_sentence)\n",
    "#     # predicted_sentence = predict_single_sentence(model=model, original_sentence=test_sentence, max_len=max_len, char_to_index=char_to_index, indicies_to_labels=indicies_to_labels, batch_size=validation_batch_size)\n",
    "#     # print(len(remove_diactrics([predicted_sentence])[0]))\n",
    "#     # print(predicted_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T10:34:10.072657Z",
     "iopub.status.busy": "2025-12-09T10:34:10.072064Z",
     "iopub.status.idle": "2025-12-09T10:34:10.076441Z",
     "shell.execute_reply": "2025-12-09T10:34:10.075898Z",
     "shell.execute_reply.started": "2025-12-09T10:34:10.072635Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import textwrap\n",
    "import re\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from torch.nn import BatchNorm1d\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import json\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import sys\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T10:34:10.661623Z",
     "iopub.status.busy": "2025-12-09T10:34:10.661121Z",
     "iopub.status.idle": "2025-12-09T10:34:10.671092Z",
     "shell.execute_reply": "2025-12-09T10:34:10.670382Z",
     "shell.execute_reply.started": "2025-12-09T10:34:10.661602Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# best_model path\n",
    "# model_path = '/kaggle/input/rnn97/pytorch/default/1/BiLSTM_Loss=0.0325396_Accuracy=97.768%_embedding_size=300hidden_size=256lr=0.001num_layers=5num_epochs=19max_len=600batch_size=32.pkl'\n",
    "\n",
    "# these are the indices of the characters in the vocabulary\n",
    "char_to_index = {'د': 1, '؟': 2, 'آ': 3, 'إ': 4, 'ؤ': 5, 'ط': 6, 'م': 7, '،': 8, 'ة': 9, 'ت': 10, 'ر': 11, 'ئ': 12, 'ا': 13, 'ض': 14, '!': 15, ' ': 16, 'ك': 17, 'غ': 18, 'س': 19, 'ص': 20, 'أ': 21, 'ل': 22, 'ف': 23, 'ظ': 24, 'ج': 25, '؛': 26, 'ن': 27, 'ع': 28, 'ب': 29, 'ث': 30, 'ه': 31, 'خ': 32, 'ى': 33, 'ء': 34, 'ز': 35, 'ق': 36, 'ي': 37, 'ش': 38, 'ح': 39, ':': 40, 'ذ': 41, 'و': 42, '.': 43}\n",
    "index_to_char = {1: 'د', 2: '؟', 3: 'آ', 4: 'إ', 5: 'ؤ', 6: 'ط', 7: 'م', 8: '،', 9: 'ة', 10: 'ت', 11: 'ر', 12: 'ئ', 13: 'ا', 14: 'ض', 15: '!', 16: ' ', 17: 'ك', 18: 'غ', 19: 'س', 20: 'ص', 21: 'أ', 22: 'ل', 23: 'ف', 24: 'ظ', 25: 'ج', 26: '؛', 27: 'ن', 28: 'ع', 29: 'ب', 30: 'ث', 31: 'ه', 32: 'خ', 33: 'ى', 34: 'ء', 35: 'ز', 36: 'ق', 37: 'ي', 38: 'ش', 39: 'ح', 40: ':', 41: 'ذ', 42: 'و', 43: '.'}\n",
    "\n",
    "# define the diacritics unicode and their corresponding labels classes indices\n",
    "# note that index 14 is reserved for no diacritic\n",
    "labels = {\n",
    "    # fath\n",
    "    1614: 0,\n",
    "    # tanween bel fath\n",
    "    1611: 1,\n",
    "    # damm\n",
    "    1615: 2,\n",
    "    # tanween bel damm\n",
    "    1612: 3,\n",
    "    # kasr\n",
    "    1616: 4,\n",
    "    # tanween bel kasr\n",
    "    1613: 5,\n",
    "    # sukun\n",
    "    1618: 6,\n",
    "    # shadd\n",
    "    1617: 7,\n",
    "    # shadd and fath\n",
    "    (1617, 1614): 8,\n",
    "    # shadd and tanween bel fath\n",
    "    (1617, 1611): 9,\n",
    "    # shadd and damm\n",
    "    (1617, 1615): 10,\n",
    "    # shadd and tanween bel damm\n",
    "    (1617, 1612): 11,\n",
    "    # shadd and kasr\n",
    "    (1617, 1616): 12,\n",
    "    # shadd and tanween bel kasr\n",
    "    (1617, 1613): 13,\n",
    "    # no diacritic\n",
    "    0: 14,\n",
    "    # padded\n",
    "    15: 15\n",
    "}\n",
    "\n",
    "indicies_to_labels = {\n",
    "    # fath\n",
    "    0: 1614,\n",
    "    # tanween bel fath\n",
    "    1: 1611,\n",
    "    # damm\n",
    "    2: 1615,\n",
    "    # tanween bel damm\n",
    "    3: 1612,\n",
    "    # kasr\n",
    "    4: 1616,\n",
    "    # tanween bel kasr\n",
    "    5: 1613,\n",
    "    # sukun\n",
    "    6: 1618,\n",
    "    # shadd\n",
    "    7: 1617,\n",
    "    # shadd and fath\n",
    "    8: (1617, 1614),\n",
    "    # shadd and tanween bel fath\n",
    "    9: (1617, 1611),\n",
    "    # shadd and damm\n",
    "    10: (1617, 1615),\n",
    "    # shadd and tanween bel damm\n",
    "    11: (1617, 1612),\n",
    "    # shadd and kasr\n",
    "    12: (1617, 1616),\n",
    "    # shadd and tanween bel kasr\n",
    "    13: (1617, 1613),\n",
    "    # no diacritic\n",
    "    14: 0,\n",
    "    # padded\n",
    "    15: 15\n",
    "}\n",
    "\n",
    "# max sentence length\n",
    "max_len = 600\n",
    "\n",
    "# batch size, number of sentences to be processed at once\n",
    "training_batch_size = 32\n",
    "validation_batch_size = 256\n",
    "\n",
    "dataset_path = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T10:38:41.999152Z",
     "iopub.status.busy": "2025-12-09T10:38:41.998609Z",
     "iopub.status.idle": "2025-12-09T10:38:42.020367Z",
     "shell.execute_reply": "2025-12-09T10:38:42.019706Z",
     "shell.execute_reply.started": "2025-12-09T10:38:41.999128Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def replace_pattern(text,pattern,replace = ''):\n",
    "    \"\"\"\n",
    "    This function replaces a pattern in a text with a string\n",
    "    Args:\n",
    "        text: string\n",
    "        pattern: regex pattern\n",
    "        replace: string to replace the pattern with\n",
    "    Returns:\n",
    "        string\n",
    "    \"\"\"\n",
    "    # Replace diacritics from the text\n",
    "    cleaned_text = pattern.sub(replace, text)\n",
    "    return cleaned_text\n",
    "\n",
    "def clean(lines):\n",
    "    \"\"\"\n",
    "    This function cleans the text from unwanted characters\n",
    "    Args:\n",
    "        lines: list of strings\n",
    "    Returns:\n",
    "        list of strings\n",
    "    \"\"\"\n",
    "    for i in range(len(lines)):\n",
    "        # remove any brackets that have only numbers inside and remove all numbers\n",
    "        reg = r'\\(\\s*(\\d+)\\s*\\)|\\(\\s*(\\d+)\\s*\\/\\s*(\\d+)\\s*\\)|\\d+'\n",
    "        lines[i] = replace_pattern(lines[i], re.compile(reg))\n",
    "        # replace all different types of brackets with a single type\n",
    "        reg_brackets = r'[\\[\\{\\(\\]\\}\\)]'\n",
    "        lines[i] = re.compile(reg_brackets).sub('', lines[i])\n",
    "        # remove some unwanted characters\n",
    "        #reg = r'[/!\\-؛،؟:\\.]'\n",
    "        reg = r'[/\\/\\\\\\-]'\n",
    "        lines[i] = replace_pattern(lines[i], re.compile(reg))\n",
    "        # remove unwanted characters\n",
    "        reg = r'[,»–\\';«*\\u200f\"\\\\~`]'\n",
    "        lines[i] = replace_pattern(lines[i], re.compile(reg))\n",
    "        # remove English characters (a-z, A-Z)\n",
    "        reg = r'[a-zA-Z]'\n",
    "        lines[i] = replace_pattern(lines[i], re.compile(reg))\n",
    "        # remove extra spaces\n",
    "        reg = r'\\s+'\n",
    "        lines[i] = replace_pattern(lines[i], re.compile(reg), ' ')\n",
    "    return lines\n",
    "\n",
    "def remove_diactrics(lines):\n",
    "    \"\"\"\n",
    "    This function removes diacritics from the text\n",
    "    Args:\n",
    "        lines: list of strings\n",
    "    Returns:\n",
    "        list of strings\n",
    "    \"\"\"\n",
    "    for i in range(len(lines)):\n",
    "        # remove diacritics\n",
    "        reg = r'[\\u064B-\\u065F\\u0670\\uFE70-\\uFE7F]'\n",
    "        lines[i] = replace_pattern(lines[i], re.compile(reg))\n",
    "    return lines\n",
    "\n",
    "def preprocess(lines, data_type, dataset_path = '', with_labels = False):\n",
    "    \"\"\"\n",
    "    This function cleans the text and saves it to a file\n",
    "    Args:\n",
    "        lines: list of strings\n",
    "        data_type: 'train', 'val', or 'test'\n",
    "        dataset_path: path to the dataset files\n",
    "        with_labels: if True, the labels will be saved to the file\n",
    "    Returns:\n",
    "        list of strings\n",
    "    \"\"\"\n",
    "    # data_type can be 'train', 'val', or 'test'\n",
    "    # clean the text from unwanted characters\n",
    "    lines = clean(lines)\n",
    "    dataset_path = '/kaggle/working/'\n",
    "    if len(lines) == 0:\n",
    "        return lines\n",
    "    if with_labels:\n",
    "        # save the cleaned text with diacritics to a file\n",
    "        with open(f'{dataset_path}cleaned_{data_type}_data_with_diacritics.txt', 'a+',encoding='utf-8') as f:\n",
    "            f.write('\\n'.join(lines))\n",
    "            f.write('\\n')\n",
    "    # remove diacritics\n",
    "    lines = remove_diactrics(lines)\n",
    "    # save the cleaned text without diacritics to a file\n",
    "    with open(f'{dataset_path}cleaned_{data_type}_data_without_diacritics.txt', 'a+',encoding='utf-8') as f:\n",
    "        f.write('\\n'.join(lines))\n",
    "        f.write('\\n')\n",
    "    return lines\n",
    "\n",
    "def preprocess_data(data_type, limit = None, dataset_path = '.', with_labels = True):\n",
    "    \"\"\"\n",
    "    This function reads the data and cleans it and saves it to files\n",
    "    Args:\n",
    "        data_type: 'train', 'val', or 'test'\n",
    "        limit: number of lines to read\n",
    "        dataset_path: path to the dataset files\n",
    "        with_labels: if True, the labels will be saved to the file\n",
    "    Returns:\n",
    "        list of strings\n",
    "    \"\"\"\n",
    "    dataset_path = '/kaggle/working/'\n",
    "    # delete the output files if exist\n",
    "    with open(f'{dataset_path}cleaned_{data_type}_data_with_diacritics.txt', 'w',encoding='utf-8') as f:\n",
    "        pass\n",
    "    with open(f'{dataset_path}cleaned_{data_type}_data_without_diacritics.txt', 'w',encoding='utf-8') as f:\n",
    "        pass\n",
    "    sentences = []\n",
    "    # read the data and clean it and save it to the files\n",
    "    dataset_path = '/kaggle/input/contest-data/'\n",
    "    with open(f'{dataset_path}{data_type}.txt', 'r',encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        lines = [line.strip() for line in lines]\n",
    "        if limit == None:\n",
    "            limit = len(lines)\n",
    "        lines = lines[:limit]\n",
    "        sentences = preprocess(lines, data_type, dataset_path, with_labels=with_labels)\n",
    "\n",
    "    return sentences\n",
    "\n",
    "def count_spaces(input_string):\n",
    "    \"\"\"\n",
    "    Count the number of spaces in a string\n",
    "    Args:\n",
    "        input_string: string to count spaces in\n",
    "    Returns:\n",
    "        space_count: number of spaces in the string\n",
    "    \"\"\"\n",
    "    \n",
    "    space_count = len(re.findall(r'\\s', input_string))\n",
    "    return space_count\n",
    "\n",
    "def tokenize_data(data_type='test', dataset_path='.', max_len=200, with_labels=True):\n",
    "    \"\"\"\n",
    "    Tokenize the data into sentences of max_len, without cutting words\n",
    "    Args:\n",
    "        data_type: 'train', 'test', or 'val'\n",
    "        dataset_path: path to the dataset folder\n",
    "        max_len: maximum length of a sentence\n",
    "        with_labels: whether to tokenize data with labels or not\n",
    "    Returns:\n",
    "        data: list of sentences without diacritics\n",
    "        data_with_diacritics: list of sentences with diacritics\n",
    "    \"\"\"\n",
    "    \n",
    "    data = []\n",
    "    spaces = []\n",
    "    # tokenize data without diacritics\n",
    "    dataset_path = '/kaggle/working/'\n",
    "    with open(f'{dataset_path}cleaned_{data_type}_data_without_diacritics.txt', 'r', encoding='utf-8') as file:\n",
    "        # read all lines into array of lines\n",
    "        data_lines = file.readlines()\n",
    "        for i in range(len(data_lines)):\n",
    "            data_lines[i] = re.compile(r'[\\n\\r\\t]').sub('', data_lines[i])\n",
    "            data_lines[i] = re.compile(r'\\s+').sub(' ', data_lines[i])\n",
    "            data_lines[i] = data_lines[i].strip()\n",
    "\n",
    "            # Split the line into sentences of max_len, without cutting words\n",
    "            sentences = textwrap.wrap(data_lines[i], max_len)\n",
    "\n",
    "            for sentence in sentences:\n",
    "                data.append(sentence)\n",
    "                spaces.append(count_spaces(sentence))\n",
    "                    \n",
    "    data_with_diacritics = []\n",
    "    \n",
    "    if with_labels:\n",
    "        spaces_index = 0\n",
    "        # tokenize data with diacritics\n",
    "        dataset_path = '/kaggle/working/'\n",
    "        with open(f'{dataset_path}cleaned_{data_type}_data_with_diacritics.txt', 'r', encoding='utf-8') as file:\n",
    "            data_with_diacritics_lines = file.readlines()\n",
    "            for i in range(len(data_with_diacritics_lines)):\n",
    "                data_with_diacritics_lines[i] = re.compile(r'[\\n\\r\\t]').sub('', data_with_diacritics_lines[i])\n",
    "                data_with_diacritics_lines[i] = re.compile(r'\\s+').sub(' ', data_with_diacritics_lines[i])\n",
    "                data_with_diacritics_lines[i] = data_with_diacritics_lines[i].strip()\n",
    "\n",
    "                remaining = data_with_diacritics_lines[i]\n",
    "                remaining_length = len(remaining)\n",
    "                while(remaining_length > 0):\n",
    "                    spaces_to_include = spaces[spaces_index]\n",
    "                    spaces_index += 1\n",
    "                    words = remaining.split()\n",
    "                    if len(words) <= spaces_to_include + 1:\n",
    "                        data_with_diacritics.append(remaining.strip())\n",
    "                        remaining_length = 0\n",
    "                        break\n",
    "                    else:\n",
    "                        sentence = ' '.join(words[:spaces_to_include + 1])\n",
    "                        data_with_diacritics.append(sentence.strip())\n",
    "                        remaining = ' '.join(words[spaces_to_include + 1:]).strip()\n",
    "                        remaining_length = len(remaining)\n",
    "                        \n",
    "    return data, data_with_diacritics\n",
    "\n",
    "def convert2idx(data=[], char_to_index={}, max_len=200, device='cpu'):\n",
    "    \"\"\"\n",
    "    Convert the data into sequences of indices\n",
    "    Args:\n",
    "        data: list of sentences\n",
    "        char_to_index: dictionary mapping characters to indices\n",
    "        max_len: maximum length of a sentence\n",
    "    Returns:\n",
    "        data_sequences: list of sequences of indices\n",
    "    \"\"\"\n",
    "    \n",
    "    # build one array that holds all sequences of data\n",
    "    data_sequences = [[char_to_index[char] for char in sequence] for sequence in data]\n",
    "    \n",
    "    # pad sequences to the maximum length\n",
    "    data_sequences = [sequence + [0] * (max_len - len(sequence)) for sequence in data_sequences]\n",
    "\n",
    "    # convert to tensor\n",
    "    data_sequences = torch.tensor(data_sequences).to(device)\n",
    "    \n",
    "    return data_sequences\n",
    "\n",
    "def label_data(data_with_diacritics=[], labels={}, max_len=200, device='cpu'):\n",
    "    \"\"\"\n",
    "    Label the data with diacritics\n",
    "    Args:\n",
    "        data_with_diacritics: list of sentences with diacritics\n",
    "        labels: dictionary mapping diacritics to indices\n",
    "        max_len: maximum length of a sentence\n",
    "    Returns:\n",
    "        data_labels: list of sequences of labels\n",
    "    \"\"\"\n",
    "    data_labels = []\n",
    "    size = len(data_with_diacritics)\n",
    "    for sentence_index in range(size):\n",
    "        sentence_labels = []\n",
    "        sentence_size = len(data_with_diacritics[sentence_index])\n",
    "        index = 0\n",
    "        while index < sentence_size:\n",
    "            if ord(data_with_diacritics[sentence_index][index]) not in labels:\n",
    "                char_sequence = char_to_index[data_with_diacritics[sentence_index][index]]\n",
    "                if char_sequence == 2 or char_sequence == 8 or char_sequence == 15 or char_sequence == 16 or char_sequence == 26 or char_sequence == 40 or char_sequence == 43:\n",
    "                    # unwanted char\n",
    "                    sentence_labels.append(14)\n",
    "                    index += 1\n",
    "                    continue\n",
    "                # char is not a diacritic\n",
    "                if (index + 1) < sentence_size and ord(data_with_diacritics[sentence_index][index + 1]) in labels:\n",
    "                    # char has a diacritic\n",
    "                    if ord(data_with_diacritics[sentence_index][index + 1]) == 1617:\n",
    "                        # char has a shadd diacritic\n",
    "                        if (index + 2) < sentence_size and ord(data_with_diacritics[sentence_index][index + 2]) in labels:\n",
    "                            # char has a shadd and another diacritic\n",
    "                            sentence_labels.append(labels[(1617, ord(data_with_diacritics[sentence_index][index + 2]))])\n",
    "                            # skip next 2 diacritics chars\n",
    "                            index += 3  # increment by 3 to skip two diacritic chars\n",
    "                            continue\n",
    "                        else:\n",
    "                            # char has a shadd and no other diacritic\n",
    "                            sentence_labels.append(labels[1617])\n",
    "                            # skip next diacritic char\n",
    "                            index += 2\n",
    "                            continue\n",
    "                    # char has a diacritic other than shadd\n",
    "                    sentence_labels.append(labels[ord(data_with_diacritics[sentence_index][index + 1])])\n",
    "                    # skip next diacritic char\n",
    "                    index += 2  # increment by 2 to skip one diacritic char\n",
    "                    continue\n",
    "                else:\n",
    "                    # char has no diacritic\n",
    "                    sentence_labels.append(14)\n",
    "            index += 1  # increment by 1 for normal iteration\n",
    "\n",
    "        data_labels.append(sentence_labels)\n",
    "\n",
    "    # pad sequences to the maximum length\n",
    "    data_labels = [sequence + [15] * (max_len - len(sequence)) for sequence in data_labels]\n",
    "\n",
    "    data_labels = torch.tensor(data_labels).to(device)\n",
    "    \n",
    "    return data_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T10:34:16.142970Z",
     "iopub.status.busy": "2025-12-09T10:34:16.142382Z",
     "iopub.status.idle": "2025-12-09T10:34:16.148319Z",
     "shell.execute_reply": "2025-12-09T10:34:16.147739Z",
     "shell.execute_reply.started": "2025-12-09T10:34:16.142946Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_dataloader(data_type='train', max_len=200, batch_size=256, dataset_path='', char_to_index={}, labels={}, device='cpu', with_labels=True):\n",
    "    \"\"\"\n",
    "    Get the data loader for the given data type\n",
    "    Args:\n",
    "        data_type: 'train', 'test', or 'val'\n",
    "        max_len: maximum length of a sentence\n",
    "        batch_size: batch size\n",
    "        dataset_path: path to the dataset folder\n",
    "        char_to_index: dictionary mapping characters to indices\n",
    "        labels: dictionary mapping diacritics to indices\n",
    "        with_labels: whether to get data with labels or not\n",
    "        device: 'cpu' or 'cuda'\n",
    "    Returns:\n",
    "        dataloader: data loader for the given data type\n",
    "    \"\"\"\n",
    "    # call the preprocess function to preprocess the data\n",
    "    preprocess_data(data_type=data_type, dataset_path=dataset_path, with_labels=with_labels)\n",
    "    # call the tokenize function to tokenize the data, this will return two lists, one with the data and the other with the data with diacritics\n",
    "    data, data_with_diacritics = tokenize_data(data_type=data_type, dataset_path=dataset_path, max_len=max_len, with_labels=with_labels)\n",
    "\n",
    "    # call the convert2idx function to convert the data to indices\n",
    "    data_sequences = convert2idx(data=data, char_to_index=char_to_index, max_len=max_len, device=device)\n",
    "\n",
    "    # call the label_data function to label the data, this will return list of lists, each list is labels indexes for a sentence\n",
    "    # in case with_labels = False, the labels will be 0 (dummy) for all characters\n",
    "    if with_labels:\n",
    "        data_labels = label_data(data_with_diacritics=data_with_diacritics, labels=labels, max_len=max_len, device=device)\n",
    "    else:\n",
    "        data_labels = torch.tensor([[15] * max_len] * len(data_sequences)).to(device)\n",
    "        \n",
    "    print(f'{data_type} data shape: ', len(data))\n",
    "    print(f'{data_type} data sequences shape: ', data_sequences.shape)\n",
    "\n",
    "    # convert the data to tensors data loader\n",
    "    dataset = TensorDataset(data_sequences, data_labels)\n",
    "\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size)\n",
    "    \n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T10:35:40.687309Z",
     "iopub.status.busy": "2025-12-09T10:35:40.686805Z",
     "iopub.status.idle": "2025-12-09T10:35:40.715239Z",
     "shell.execute_reply": "2025-12-09T10:35:40.714539Z",
     "shell.execute_reply.started": "2025-12-09T10:35:40.687287Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class CharLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    This class implements the character level BiLSTM model\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size, output_size, dropout_rate, num_layers=1):\n",
    "        super(CharLSTM, self).__init__()\n",
    "        # chars embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "\n",
    "        # LSTM layers\n",
    "        # batch_first: it means that the input tensor has its first dimension representing the batch size\n",
    "        self.lstm = nn.LSTM(input_size=embedding_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True, bidirectional=True, dropout=dropout_rate)\n",
    "        \n",
    "        # batch normalization layer, to normalize the hidden states, it simply does the following:\n",
    "        # x = (x - mean) / std\n",
    "        # where mean and std are calculated for each hidden state\n",
    "\n",
    "        self.batchnorm = BatchNorm1d(max_len)\n",
    "\n",
    "        # output layer, final_output = W * concatenated_hidden_states + bias\n",
    "        self.output = nn.Linear(hidden_size * 2, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the model\n",
    "        Args:\n",
    "            x: input tensor\n",
    "        Returns:\n",
    "            output: output tensor\n",
    "        \"\"\"\n",
    "        embedded = self.embedding(x) # batch_size * seq_length * embedding_size\n",
    "        lstm_out, _ = self.lstm(embedded) # batch_size * seq_length * hidden_size\n",
    "        lstm_out = self.batchnorm(lstm_out) # batch_size * seq_length * hidden_size\n",
    "        output = self.output(lstm_out)  # batch_size * seq_length * output_size\n",
    "        return output\n",
    "    \n",
    "def train(model=None, start_epoch=0, optimizer=None, scheduler=None, num_epochs=19, checkpoint_path='checkpoint.pth'):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"); print(device)\n",
    "    training_dataloader = get_dataloader(data_type='train', max_len=max_len, batch_size=training_batch_size, dataset_path=dataset_path, char_to_index=char_to_index, labels=labels, device=device, with_labels=True)\n",
    "    validation_dataloader = get_dataloader(data_type='val', max_len=max_len, batch_size=validation_batch_size, dataset_path=dataset_path, char_to_index=char_to_index, labels=labels, device=device, with_labels=True)\n",
    "    return None\n",
    "    if model is None:\n",
    "        vocab_size = len(char_to_index) + 1\n",
    "        embedding_size = 300\n",
    "        hidden_size = 256\n",
    "        output_size = len(labels)\n",
    "        dropout_rate = 0.2\n",
    "        num_layers = 5\n",
    "        lr = 0.001\n",
    "        model = CharLSTM(vocab_size, embedding_size, hidden_size, output_size, dropout_rate, num_layers).to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        # scheduler = StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "    else:\n",
    "        # If continuing training, make sure model is on device\n",
    "        model.to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=15)\n",
    "\n",
    "    best_f1 = -1\n",
    "    best_model_state = None\n",
    "    print('Train starts')\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        model.train()\n",
    "        train_correct, train_total = 0, 0\n",
    "        train_preds, train_trues = [], []\n",
    "        # train the model for one epoch\n",
    "        for batch_sequences, batch_labels in training_dataloader:\n",
    "            # zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass\n",
    "            outputs = model(batch_sequences) # batch_size * seq_length * output_size\n",
    "            # calculate loss\n",
    "            # outputs: batch_size, seq_length, output_size\n",
    "            # labels: batch_size, seq_length\n",
    "            # reshape (flatten) the outputs and labels to be 2D\n",
    "            # outputs: batch_size * seq_length, output_size\n",
    "            # labels: batch_size * seq_length\n",
    "            flat_outputs = outputs.view(-1, outputs.shape[-1])\n",
    "            flat_labels = batch_labels.view(-1)\n",
    "            mask = (flat_labels != 15)\n",
    "            loss = criterion(flat_outputs[mask], flat_labels[mask])\n",
    "            # backward pass\n",
    "            loss.backward()\n",
    "            # update parameters\n",
    "            optimizer.step()\n",
    "            \n",
    "            pred = flat_outputs.argmax(dim=1)\n",
    "\n",
    "            train_correct += (pred[mask] == flat_labels[mask]).sum().item()\n",
    "            train_total += mask.sum().item()\n",
    "    \n",
    "            # store for F1\n",
    "            train_preds.extend(pred[mask].cpu().tolist())\n",
    "            train_trues.extend(flat_labels[mask].cpu().tolist())\n",
    "\n",
    "        \n",
    "        train_accuracy = train_correct / train_total\n",
    "        train_f1 = f1_score(train_trues, train_preds, average='macro')\n",
    "        last_loss = loss.item()\n",
    "            \n",
    "        # validate the model\n",
    "        model.eval()\n",
    "        val_correct, val_total = 0, 0\n",
    "        val_preds, val_trues = [], []\n",
    "        with torch.inference_mode():\n",
    "            for val_seq, val_label in validation_dataloader:\n",
    "                outputs = model(val_seq)\n",
    "                pred = outputs.argmax(dim=2)\n",
    "                flat_pred = pred.view(-1)\n",
    "                flat_labels = val_label.view(-1)\n",
    "                mask = (flat_labels != 15)\n",
    "\n",
    "                val_correct += (flat_pred[mask] == flat_labels[mask]).sum().item()\n",
    "                val_total += mask.sum().item()\n",
    "                val_preds.extend(flat_pred[mask].cpu().tolist())\n",
    "                val_trues.extend(flat_labels[mask].cpu().tolist())\n",
    "    \n",
    "        val_accuracy = val_correct / val_total\n",
    "        val_f1 = f1_score(val_trues, val_preds, average='macro')\n",
    "\n",
    "        \n",
    "        # decrease the learning rate\n",
    "        # scheduler.step()\n",
    "        \n",
    "        print(\n",
    "            f\"Epoch {epoch+1}/{num_epochs} | \"\n",
    "            f\"Loss: {last_loss:.5f} | \"\n",
    "            f\"Train Acc: {train_accuracy*100:.2f}% | Train F1: {train_f1:.3f} | \"\n",
    "            f\"Val Acc: {val_accuracy*100:.2f}% | Val F1: {val_f1:.3f}\"\n",
    "        )\n",
    "        # SAVE BEST MODEL STATE\n",
    "        if val_f1 > best_f1:\n",
    "            best_f1 = val_f1\n",
    "            best_model_state = model.state_dict()\n",
    "            print(f\"New best model saved (Val F1={best_f1:.3f})\")\n",
    "\n",
    "        if checkpoint_path is not None:\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state': model.state_dict(),\n",
    "                'optimizer_state': optimizer.state_dict(),\n",
    "                # 'scheduler_state': scheduler.state_dict(),\n",
    "                'best_f1': best_f1\n",
    "            }, checkpoint_path)\n",
    "            \n",
    "            \n",
    "    #  SAVE THE BEST MODEL\n",
    "    model_path = \"best_model.pkl\"\n",
    "    meta_path = \"best_model_meta.json\"\n",
    "    \n",
    "    with open(model_path, \"wb\") as f:\n",
    "        pickle.dump(best_model_state, f)\n",
    "    \n",
    "    metadata = {\n",
    "        \"model_type\": \"CharLSTM\",\n",
    "        \"best_f1\": float(best_f1),\n",
    "        \"embedding_size\": embedding_size,\n",
    "        \"hidden_size\": hidden_size,\n",
    "        \"num_layers\": num_layers,\n",
    "        \"dropout_rate\": dropout_rate,\n",
    "        \"learning_rate\": lr,\n",
    "        \"num_epochs\": num_epochs,\n",
    "        \"max_sequence_length\": max_len,\n",
    "        \"batch_size\": training_batch_size,\n",
    "        \"vocab_size\": vocab_size,\n",
    "        \"output_classes\": output_size,\n",
    "        \"model_file\": model_path\n",
    "    }\n",
    "    \n",
    "    with open(meta_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(metadata, f, ensure_ascii=False, indent=4)\n",
    "    \n",
    "    print(\"\\nTraining Finished!\")\n",
    "    print(f\"Best model saved to: {model_path}\")\n",
    "    print(f\"Metadata saved to: {meta_path}\")\n",
    "\n",
    "def load_model(model_path=\"best_model.pkl\", meta_path=\"best_model_meta.json\", device=None):\n",
    "    \"\"\"\n",
    "    Load the model from a saved state dict and metadata.\n",
    "\n",
    "    Returns:\n",
    "        model: loaded CharLSTM model\n",
    "        metadata: dictionary with hyperparameters\n",
    "    \"\"\"\n",
    "    # load the model\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # base = ''\n",
    "    base = '/kaggle/working/'\n",
    "    # Load metadata\n",
    "    with open(base + meta_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        metadata = json.load(f)\n",
    "\n",
    "    # Recreate model\n",
    "    model = CharLSTM(\n",
    "        vocab_size=metadata[\"vocab_size\"],\n",
    "        embedding_size=metadata[\"embedding_size\"],\n",
    "        hidden_size=metadata[\"hidden_size\"],\n",
    "        output_size=metadata[\"output_classes\"],\n",
    "        dropout_rate=metadata[\"dropout_rate\"],\n",
    "        num_layers=metadata[\"num_layers\"]\n",
    "    ).to(device)\n",
    "\n",
    "    # Load weights\n",
    "    with open(base + model_path, \"rb\") as f:\n",
    "        state_dict = pickle.load(f)\n",
    "        \n",
    "    model.load_state_dict(state_dict)\n",
    "    model.to(device)\n",
    "\n",
    "    return model, metadata\n",
    "    \n",
    "def load_checkpoint(checkpoint_path=\"checkpoint.pth\", device=None):\n",
    "    \"\"\"\n",
    "    Load model, optimizer, scheduler, last epoch and best F1 to continue training.\n",
    "    Returns model, optimizer, scheduler, start_epoch, best_f1\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    metadata_path = \"best_model_meta.json\"\n",
    "    with open(metadata_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        metadata = json.load(f)\n",
    "\n",
    "    model = CharLSTM(\n",
    "        vocab_size=metadata[\"vocab_size\"],\n",
    "        embedding_size=metadata[\"embedding_size\"],\n",
    "        hidden_size=metadata[\"hidden_size\"],\n",
    "        output_size=metadata[\"output_classes\"],\n",
    "        dropout_rate=metadata[\"dropout_rate\"],\n",
    "        num_layers=metadata[\"num_layers\"]\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=metadata[\"learning_rate\"])\n",
    "    # scheduler = StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "    model.load_state_dict(checkpoint['model_state'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state'])\n",
    "    # scheduler.load_state_dict(checkpoint['scheduler_state'])\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    best_f1 = checkpoint['best_f1']\n",
    "\n",
    "    return model, optimizer, start_epoch, best_f1\n",
    "\n",
    "def predict_test(model):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"); print(device)\n",
    "    \n",
    "    test_dataloader = get_dataloader(data_type='test', max_len=max_len, batch_size=validation_batch_size, dataset_path=dataset_path, char_to_index=char_to_index, labels=labels, device=device, with_labels=False)\n",
    "        \n",
    "    # open csv file to write the predictions to, with the first row as the header, ID, and label\n",
    "    with open('test.csv', 'w', encoding='utf-8') as file:\n",
    "        file.write('ID,label')\n",
    "\n",
    "        predicted_labels = []\n",
    "        \n",
    "        # make the model predict\n",
    "        model.eval()\n",
    "        for test_batch_sequences, _ in test_dataloader:\n",
    "            outputs = model(test_batch_sequences) # batch_size * seq_length * output_size\n",
    "            # Calculate accuracy\n",
    "            batch_predicted_labels = outputs.argmax(dim=2)  # Get the index with the maximum probability\n",
    "            mask = (test_batch_sequences != 0) & (test_batch_sequences != 2) & (test_batch_sequences != 8) & (test_batch_sequences != 15) & (test_batch_sequences != 16) & (test_batch_sequences != 26) & (test_batch_sequences != 40) & (test_batch_sequences != 43)\n",
    "            batch_predicted_labels = batch_predicted_labels[mask]\n",
    "            \n",
    "            # extend these predictions to the predicted_labels list\n",
    "            predicted_labels.extend(batch_predicted_labels.tolist())\n",
    "            \n",
    "        print('predicted_labels length: ', len(predicted_labels))\n",
    "        \n",
    "        # write the predictions to the file\n",
    "        for i in range(len(predicted_labels)):\n",
    "            file.write(f'\\n{i},{predicted_labels[i]}')\n",
    "        print('done')\n",
    "            \n",
    "def predict_single_sentence(model, original_sentence='', max_len=200, char_to_index={}, indicies_to_labels={}, batch_size=256):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") #; print(device)\n",
    "    # preprocess the original sentence\n",
    "    preprocessed_sentence = original_sentence.strip()\n",
    "    preprocessed_sentence = clean([preprocessed_sentence])[0]\n",
    "    \n",
    "    # tokenize the sentence\n",
    "    preprocessed_sentence = re.compile(r'[\\n\\r\\t]').sub('', preprocessed_sentence)\n",
    "    preprocessed_sentence = re.compile(r'\\s+').sub(' ', preprocessed_sentence)\n",
    "    preprocessed_sentence = preprocessed_sentence.strip()\n",
    "\n",
    "    tokenized_sentences = []\n",
    "    \n",
    "    # split the line into sentences by dot\n",
    "    dot_splitted_list = preprocessed_sentence.split('.')\n",
    "\n",
    "    # remove last string if empty\n",
    "    if dot_splitted_list[-1] == '':\n",
    "        dot_splitted_list = dot_splitted_list[:-1]\n",
    "\n",
    "    for dot_splitted in dot_splitted_list:\n",
    "        dot_splitted = dot_splitted.strip()\n",
    "        # Split the line into sentences of max_len, without cutting words\n",
    "        sentences = textwrap.wrap(dot_splitted, max_len)\n",
    "\n",
    "        for sentence in sentences:\n",
    "            tokenized_sentences.append(sentence)\n",
    "            \n",
    "    sentence_sequences = convert2idx(data=tokenized_sentences, char_to_index=char_to_index, max_len=max_len, device=device)\n",
    "    \n",
    "    dataset = TensorDataset(sentence_sequences, sentence_sequences)\n",
    "\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size)\n",
    "\n",
    "    predicted_labels = []\n",
    "        \n",
    "    # make the model predict\n",
    "    model.eval()\n",
    "    for batch_sequences, batch_labels in dataloader:\n",
    "        outputs = model(batch_sequences) # batch_size * seq_length * output_size\n",
    "        # Calculate accuracy\n",
    "        batch_predicted_labels = outputs.argmax(dim=2)  # Get the index with the maximum probability\n",
    "        mask = (batch_labels != 15) & (batch_sequences != 2) & (batch_sequences != 8) & (batch_sequences != 15) & (batch_sequences != 16) & (batch_sequences != 26) & (batch_sequences != 40) & (batch_sequences != 43)\n",
    "        batch_predicted_labels = batch_predicted_labels[mask]\n",
    "        \n",
    "        # extend these predictions to the predicted_labels list\n",
    "        predicted_labels.extend(batch_predicted_labels.tolist())\n",
    "        \n",
    "    predicted_sentence = \"\"\n",
    "    predicted_char_index = 0\n",
    "    for char in original_sentence:\n",
    "        predicted_sentence += char\n",
    "        # if the char is an unknown char, ., space, ?, ... or :, then keep it as it is\n",
    "        if char not in char_to_index:\n",
    "            continue\n",
    "        elif char_to_index[char] == 2 or char_to_index[char] == 8 or char_to_index[char] == 15 or char_to_index[char] == 16 or char_to_index[char] == 26 or char_to_index[char] == 40 or char_to_index[char] == 43:\n",
    "            continue\n",
    "        # get it predicted diacritic (char or tuple of chars)\n",
    "        predicted_class = indicies_to_labels[predicted_labels[predicted_char_index]]\n",
    "        if type(predicted_class) is tuple:\n",
    "            predicted_sentence += chr(predicted_class[0]) + chr(predicted_class[1])\n",
    "            predicted_char_index += 1\n",
    "        elif predicted_class == 0:\n",
    "            # if the predicted diacritic is 0 (no diacritic), then keep the char as it is\n",
    "            predicted_char_index += 1\n",
    "        else:\n",
    "            # else, add the predicted diacritic to the char\n",
    "            predicted_sentence += chr(predicted_class)\n",
    "            predicted_char_index += 1\n",
    "                \n",
    "    return predicted_sentence\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T10:35:44.055984Z",
     "iopub.status.busy": "2025-12-09T10:35:44.055294Z",
     "iopub.status.idle": "2025-12-09T10:35:44.059637Z",
     "shell.execute_reply": "2025-12-09T10:35:44.059078Z",
     "shell.execute_reply.started": "2025-12-09T10:35:44.055961Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # main\n",
    "# if __name__ == \"__main__\":\n",
    "# # #     # supress warnings\n",
    "# # #     warnings.filterwarnings('ignore')\n",
    "    \n",
    "# # #     # NOTE: Uncomment this line to train the model\n",
    "#     dataset_path = ''\n",
    "    # train()\n",
    "# # #     # load the model\n",
    "#     # model_path = 'best_model.pkl'\n",
    "#     # model, _ = load_model(model_path)\n",
    "    \n",
    "# #     # predict whole test data, and output labels to submission.csv\n",
    "#     # predict_test(model)\n",
    "    \n",
    "# #     # predict a single sentence\n",
    "# #     test_sentence = \"ليس للوكيل بالقبض أن يبرأ المدين أو يهب الدين له أو يأخذ رهنا من المدين في مقابل الدين أو يقبل إحالته على شخص آخر لكن له أن يأخذ كفيلا لكن ليس له أن يأخذ كفيلا بشرط براءة الأصيل انظر المادة ( 648 ) ( الأنقروي ، الطحطاوي وصرة الفتاوى ، البحر ) .\"\n",
    "# #     print(len(test_sentence))\n",
    "# #     print(test_sentence)\n",
    "# #     predicted_sentence = predict_single_sentence(model=model, original_sentence=test_sentence, max_len=max_len, char_to_index=char_to_index, indicies_to_labels=indicies_to_labels, batch_size=validation_batch_size)\n",
    "# #     print(len(remove_diactrics([predicted_sentence])[0]))\n",
    "# #     print(predicted_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T10:35:44.452239Z",
     "iopub.status.busy": "2025-12-09T10:35:44.451797Z",
     "iopub.status.idle": "2025-12-09T10:35:44.458197Z",
     "shell.execute_reply": "2025-12-09T10:35:44.457473Z",
     "shell.execute_reply.started": "2025-12-09T10:35:44.452217Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def test():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(device)\n",
    "    print(\"test preprocessing\")\n",
    "    \n",
    "    test_dataloader = get_dataloader(data_type='train', max_len=max_len, batch_size=validation_batch_size, dataset_path=dataset_path, char_to_index=char_to_index, labels=labels, device=device, with_labels=True)\n",
    "\n",
    "    \n",
    "    model, _ = load_model(model_path)\n",
    "    model.eval()\n",
    "    \n",
    "    # Initialize counters before the loop\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        for test_batch_sequences, test_batch_labels in tqdm(test_dataloader, desc=\"Test\", leave=True):\n",
    "            outputs = model(test_batch_sequences)\n",
    "            predicted_labels = outputs.argmax(dim=2)\n",
    "            \n",
    "            # Create mask to exclude padding and punctuation\n",
    "            mask = (\n",
    "                (test_batch_labels != 15) & \n",
    "                (test_batch_sequences != 2) & \n",
    "                (test_batch_sequences != 8) & \n",
    "                (test_batch_sequences != 15) & \n",
    "                (test_batch_sequences != 16) & \n",
    "                (test_batch_sequences != 26) & \n",
    "                (test_batch_sequences != 40) & \n",
    "                (test_batch_sequences != 43)\n",
    "            )\n",
    "            \n",
    "            correct_predictions += ((predicted_labels == test_batch_labels) & mask).sum().item()\n",
    "            total_predictions += mask.sum().item()\n",
    "    \n",
    "    # Calculate accuracy after the loop\n",
    "    accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0\n",
    "    print(f'Test Accuracy: {accuracy * 100:.3f}%')\n",
    "    \n",
    "    return predicted_labels, test_batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T10:35:47.138390Z",
     "iopub.status.busy": "2025-12-09T10:35:47.137731Z",
     "iopub.status.idle": "2025-12-09T10:35:47.171661Z",
     "shell.execute_reply": "2025-12-09T10:35:47.170753Z",
     "shell.execute_reply.started": "2025-12-09T10:35:47.138368Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "test preprocessing\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/kaggle/input/contest-data/train.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_47/113947465.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# new val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_47/2041316031.py\u001b[0m in \u001b[0;36mtest\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"test preprocessing\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mtest_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_dataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar_to_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchar_to_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwith_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_47/1995312761.py\u001b[0m in \u001b[0;36mget_dataloader\u001b[0;34m(data_type, max_len, batch_size, dataset_path, char_to_index, labels, device, with_labels)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \"\"\"\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# call the preprocess function to preprocess the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mpreprocess_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwith_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwith_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0;31m# call the tokenize function to tokenize the data, this will return two lists, one with the data and the other with the data with diacritics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_with_diacritics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenize_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwith_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwith_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_47/1757764114.py\u001b[0m in \u001b[0;36mpreprocess_data\u001b[0;34m(data_type, limit, dataset_path, with_labels)\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;31m# read the data and clean it and save it to the files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0mdataset_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/kaggle/input/contest-data/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{dataset_path}{data_type}.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/contest-data/train.txt'"
     ]
    }
   ],
   "source": [
    "# new val\n",
    "out = test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T03:33:57.285554Z",
     "iopub.status.busy": "2025-12-09T03:33:57.284932Z",
     "iopub.status.idle": "2025-12-09T03:37:32.027422Z",
     "shell.execute_reply": "2025-12-09T03:37:32.026582Z",
     "shell.execute_reply.started": "2025-12-09T03:33:57.285532Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "test preprocessing\n",
      "train data shape:  53673\n",
      "train data sequences shape:  torch.Size([53673, 600])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test: 100%|██████████| 210/210 [03:04<00:00,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 98.840%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# new train\n",
    "out = test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T03:39:37.665967Z",
     "iopub.status.busy": "2025-12-09T03:39:37.665447Z",
     "iopub.status.idle": "2025-12-09T03:39:49.103152Z",
     "shell.execute_reply": "2025-12-09T03:39:49.102530Z",
     "shell.execute_reply.started": "2025-12-09T03:39:37.665945Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "test preprocessing\n",
      "val data shape:  2696\n",
      "val data sequences shape:  torch.Size([2696, 600])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test: 100%|██████████| 11/11 [00:08<00:00,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 98.734%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# old val\n",
    "out = test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T03:40:41.507294Z",
     "iopub.status.busy": "2025-12-09T03:40:41.506715Z",
     "iopub.status.idle": "2025-12-09T03:44:16.095186Z",
     "shell.execute_reply": "2025-12-09T03:44:16.094436Z",
     "shell.execute_reply.started": "2025-12-09T03:40:41.507272Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "test preprocessing\n",
      "train data shape:  53670\n",
      "train data sequences shape:  torch.Size([53670, 600])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test: 100%|██████████| 210/210 [03:04<00:00,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 98.748%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# old train\n",
    "out = test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T03:48:56.695900Z",
     "iopub.status.busy": "2025-12-09T03:48:56.695322Z",
     "iopub.status.idle": "2025-12-09T03:48:56.705840Z",
     "shell.execute_reply": "2025-12-09T03:48:56.705105Z",
     "shell.execute_reply.started": "2025-12-09T03:48:56.695862Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def test_last_char_text(model, dataset_path='', max_len=600, batch_size=256,\n",
    "                        char_to_index=char_to_index, indicies_to_labels=indicies_to_labels, labels=labels):\n",
    "    \"\"\"\n",
    "    Compute last-character accuracy and return each word with its last character label as text.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(device)\n",
    "\n",
    "    val_loader = get_dataloader(data_type='train', max_len=max_len, batch_size=validation_batch_size,\n",
    "                                     dataset_path=dataset_path, char_to_index=char_to_index, labels=labels,\n",
    "                                     device=device, with_labels=True)\n",
    "    model_path = 'best_model.pkl'\n",
    "    model, _ = load_model(model_path)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    total_last_char = 0\n",
    "    correct_last_char = 0\n",
    "    words_text = []\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for batch_sequences, batch_labels in tqdm(val_loader, desc=\"Last Char Test\"):\n",
    "            outputs = model(batch_sequences)  # batch_size * seq_length * output_size\n",
    "            predicted_labels = outputs.argmax(dim=2)\n",
    "\n",
    "            batch_size_seq = batch_sequences.shape[0]\n",
    "\n",
    "            for i in range(batch_size_seq):\n",
    "                seq = batch_sequences[i]\n",
    "                true_labels = batch_labels[i]\n",
    "                pred_labels = predicted_labels[i]\n",
    "\n",
    "                word = ''\n",
    "                last_char_true = None\n",
    "                last_char_pred = None\n",
    "\n",
    "                for idx, c in enumerate(seq):\n",
    "                    if c in [0, 2, 8, 15, 16, 26, 40, 43]:  # padding/unwanted chars\n",
    "                        if word:\n",
    "                            last_char_true_val = last_char_true if last_char_true is not None else 0\n",
    "                            last_char_pred_val = last_char_pred if last_char_pred is not None else 0\n",
    "                            words_text.append(f\"{word}:{last_char_true_val}->{last_char_pred_val}\")\n",
    "                            if last_char_true_val == last_char_pred_val:\n",
    "                                correct_last_char += 1\n",
    "                            total_last_char += 1\n",
    "\n",
    "                            word = ''\n",
    "                            last_char_true = None\n",
    "                            last_char_pred = None\n",
    "                        continue\n",
    "\n",
    "                    word += index_to_char[int(c)]\n",
    "                    last_char_true = indicies_to_labels[int(true_labels[idx])]\n",
    "                    last_char_pred = indicies_to_labels[int(pred_labels[idx])]\n",
    "\n",
    "                # catch last word in sequence\n",
    "                if word:\n",
    "                    last_char_true_val = last_char_true if last_char_true is not None else 0\n",
    "                    last_char_pred_val = last_char_pred if last_char_pred is not None else 0\n",
    "                    words_text.append(f\"{word}:{last_char_true_val}->{last_char_pred_val}\")\n",
    "                    if last_char_true_val == last_char_pred_val:\n",
    "                        correct_last_char += 1\n",
    "                    total_last_char += 1\n",
    "\n",
    "    accuracy = correct_last_char / total_last_char if total_last_char > 0 else 0\n",
    "    print(f\"Last Character Accuracy: {accuracy*100:.3f}%\")\n",
    "\n",
    "    # return accuracy and all words as a single text string\n",
    "    return accuracy, \"\\n\".join(words_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T10:36:32.952228Z",
     "iopub.status.busy": "2025-12-09T10:36:32.951547Z",
     "iopub.status.idle": "2025-12-09T10:36:35.534128Z",
     "shell.execute_reply": "2025-12-09T10:36:35.533518Z",
     "shell.execute_reply.started": "2025-12-09T10:36:32.952206Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_path = 'best_model.pkl'\n",
    "model, _ = load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T10:43:23.210386Z",
     "iopub.status.busy": "2025-12-09T10:43:23.210129Z",
     "iopub.status.idle": "2025-12-09T10:43:23.216530Z",
     "shell.execute_reply": "2025-12-09T10:43:23.215897Z",
     "shell.execute_reply.started": "2025-12-09T10:43:23.210367Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def clean(lines):\n",
    "    \"\"\"\n",
    "    This function cleans the text from unwanted characters\n",
    "    Args:\n",
    "        lines: list of strings\n",
    "    Returns:\n",
    "        list of strings\n",
    "    \"\"\"\n",
    "    for i in range(len(lines)):\n",
    "        # remove any brackets that have only numbers inside and remove all numbers\n",
    "        reg = r'\\(\\s*(\\d+)\\s*\\)|\\(\\s*(\\d+)\\s*\\/\\s*(\\d+)\\s*\\)|\\d+'\n",
    "        lines[i] = replace_pattern(lines[i], re.compile(reg))\n",
    "        # replace all different types of brackets with a single type\n",
    "        reg_brackets = r'[\\[\\{\\(\\]\\}\\)]'\n",
    "        lines[i] = re.compile(reg_brackets).sub('', lines[i])\n",
    "        # remove some unwanted characters\n",
    "        #reg = r'[/!\\-؛،؟:\\.]'\n",
    "        reg = r'[/\\/\\\\\\-]'\n",
    "        lines[i] = replace_pattern(lines[i], re.compile(reg))\n",
    "        # remove unwanted characters\n",
    "        reg = r'[,»–\\';«*\\u200f\\u200d\\u200b\\u200c\\u200e\"\\\\~`%…_]'\n",
    "        lines[i] = replace_pattern(lines[i], re.compile(reg))\n",
    "        # remove English characters (a-z, A-Z)\n",
    "        reg = r'[a-zA-Z]'\n",
    "        lines[i] = replace_pattern(lines[i], re.compile(reg))\n",
    "        # remove fractions and superscripts/subscripts\n",
    "        reg = r'[\\u00BC-\\u00BE\\u2150-\\u215E]'\n",
    "        lines[i] = replace_pattern(lines[i], re.compile(reg))\n",
    "        # remove emojis and other symbols\n",
    "        reg = r'[\\U0001F000-\\U0001FFFF]'\n",
    "        lines[i] = replace_pattern(lines[i], re.compile(reg))\n",
    "        # remove gender symbols and similar miscellaneous symbols\n",
    "        reg = r'[\\u2600-\\u26FF\\u2700-\\u27BF]'\n",
    "        lines[i] = replace_pattern(lines[i], re.compile(reg))\n",
    "        # remove extra spaces\n",
    "        reg = r'\\s+'\n",
    "        lines[i] = replace_pattern(lines[i], re.compile(reg), ' ')\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T10:43:23.887282Z",
     "iopub.status.busy": "2025-12-09T10:43:23.887046Z",
     "iopub.status.idle": "2025-12-09T10:43:33.468527Z",
     "shell.execute_reply": "2025-12-09T10:43:33.467872Z",
     "shell.execute_reply.started": "2025-12-09T10:43:23.887265Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "test data shape:  2538\n",
      "test data sequences shape:  torch.Size([2538, 600])\n",
      "predicted_labels length:  237240\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "predict_test(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T10:49:55.778260Z",
     "iopub.status.busy": "2025-12-09T10:49:55.777655Z",
     "iopub.status.idle": "2025-12-09T10:49:55.793398Z",
     "shell.execute_reply": "2025-12-09T10:49:55.792725Z",
     "shell.execute_reply.started": "2025-12-09T10:49:55.778240Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56731</th>\n",
       "      <td>237224</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56732</th>\n",
       "      <td>237228</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56733</th>\n",
       "      <td>237231</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56734</th>\n",
       "      <td>237236</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56735</th>\n",
       "      <td>237239</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>56736 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           ID  label\n",
       "0           1     14\n",
       "1           7      4\n",
       "2          12     12\n",
       "3          18      4\n",
       "4          20      6\n",
       "...       ...    ...\n",
       "56731  237224     14\n",
       "56732  237228      2\n",
       "56733  237231     14\n",
       "56734  237236      4\n",
       "56735  237239      3\n",
       "\n",
       "[56736 rows x 2 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "test = pd.read_csv('/kaggle/working/saved_data.csv')\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T10:44:38.761372Z",
     "iopub.status.busy": "2025-12-09T10:44:38.760716Z",
     "iopub.status.idle": "2025-12-09T10:44:38.797438Z",
     "shell.execute_reply": "2025-12-09T10:44:38.796854Z",
     "shell.execute_reply.started": "2025-12-09T10:44:38.761347Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237235</th>\n",
       "      <td>237235</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237236</th>\n",
       "      <td>237236</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237237</th>\n",
       "      <td>237237</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237238</th>\n",
       "      <td>237238</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237239</th>\n",
       "      <td>237239</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>237240 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID  label\n",
       "0            0      4\n",
       "1            1     14\n",
       "2            2     14\n",
       "3            3      6\n",
       "4            4      0\n",
       "...        ...    ...\n",
       "237235  237235      6\n",
       "237236  237236      4\n",
       "237237  237237      0\n",
       "237238  237238      6\n",
       "237239  237239      3\n",
       "\n",
       "[237240 rows x 2 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gold = pd.read_csv('test.csv')\n",
    "gold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T10:48:32.906474Z",
     "iopub.status.busy": "2025-12-09T10:48:32.905704Z",
     "iopub.status.idle": "2025-12-09T10:48:32.918026Z",
     "shell.execute_reply": "2025-12-09T10:48:32.917309Z",
     "shell.execute_reply.started": "2025-12-09T10:48:32.906448Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237224</th>\n",
       "      <td>237224</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237228</th>\n",
       "      <td>237228</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237231</th>\n",
       "      <td>237231</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237236</th>\n",
       "      <td>237236</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237239</th>\n",
       "      <td>237239</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>56736 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID  label\n",
       "1            1     14\n",
       "7            7      4\n",
       "12          12     12\n",
       "18          18      4\n",
       "20          20      6\n",
       "...        ...    ...\n",
       "237224  237224     14\n",
       "237228  237228      2\n",
       "237231  237231     14\n",
       "237236  237236      4\n",
       "237239  237239      3\n",
       "\n",
       "[56736 rows x 2 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gold[test.case_ending]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T10:49:36.575050Z",
     "iopub.status.busy": "2025-12-09T10:49:36.574308Z",
     "iopub.status.idle": "2025-12-09T10:49:36.621797Z",
     "shell.execute_reply": "2025-12-09T10:49:36.621069Z",
     "shell.execute_reply.started": "2025-12-09T10:49:36.575027Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "gold[test.case_ending].to_csv(\"saved_data.csv\", index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T03:44:44.305144Z",
     "iopub.status.busy": "2025-12-09T03:44:44.304546Z",
     "iopub.status.idle": "2025-12-09T03:47:03.781755Z",
     "shell.execute_reply": "2025-12-09T03:47:03.781084Z",
     "shell.execute_reply.started": "2025-12-09T03:44:44.305120Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "val data shape:  2696\n",
      "val data sequences shape:  torch.Size([2696, 600])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Last Char Test: 100%|██████████| 11/11 [02:15<00:00, 12.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last Character Accuracy: 98.121%\n",
      "0.981209812758094\n",
      "قوله:1615->1615\n",
      "ولا:0->0\n",
      "تكره:1615->1615\n",
      "ضيافته:1615->1615\n",
      "الفرق:1615->1615\n",
      "الثالث:1615->1615\n",
      "والثلاثون:1614->1614\n",
      "بين:1614->1614\n",
      "قاعدة:1616->1616\n",
      "تقدم:1616->1616\n",
      "الحكم:1616->1616\n",
      "على:0->0\n",
      "سببه:1616->1616\n",
      "دون:1614->1614\n",
      "شرطه:1616->1616\n",
      "أو:1618->1618\n",
      "شرطه:1616->1616\n",
      "دون:1614->1614\n",
      "سببه:1616->1616\n",
      "وبين:1614->1614\n",
      "قاعدة:1616->1616\n",
      "تقدمه:1616->1616\n",
      "على:0->0\n",
      "السبب:1616->1616\n",
      "والشرط:1616->1616\n",
      "جميعا:0->0\n",
      "وتحريره:1615->1615\n",
      "أن:(1617, 1614)->(1617, 1614)\n",
      "الحكم:1614->1614\n",
      "إن:1618->1618\n",
      "كان:1614->1614\n",
      "له:1615->1615\n",
      "سبب:1612->1612\n",
      "بغير:1616->1616\n",
      "شرط:1613->1613\n",
      "فتقدم:1614->1615\n",
      "عليه:1616->1616\n",
      "لا:0->0\n",
      "يعتبر:1615->1615\n",
      "أو:1618->1618\n",
      "كان:1614->1614\n",
      "له:1615->1615\n",
      "سببان:1616->1616\n",
      "أو:1618->1618\n",
      "أسباب:1612->1612\n",
      "فتقدم:1614->1615\n",
      "على:0->0\n",
      "جميعها:0->0\n",
      "لم:1618->1618\n",
      "يعتبر:1618->1618\n",
      "أو:1618->1618\n",
      "على:0->0\n",
      "بعضها:0->0\n",
      "دون:1614->1614\n",
      "بعض:1613->1613\n",
      "اعتبر:1614->1614\n",
      "بناء:1611->1611\n",
      "على:0->0\n",
      "سبب:1616->1616\n",
      "الخاص:(1617, 1616)->(1617, 1616)\n",
      "ولا:0->0\n",
      "يضر:(1617, 1615)->(1617, 1615)\n",
      "فقدان:1615->1615\n",
      "بقية:1616->161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "accuracy, words_text = test_last_char_text(model, dataset_path='/kaggle/input/project-data/data/')\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T03:49:01.902247Z",
     "iopub.status.busy": "2025-12-09T03:49:01.901982Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "train data shape:  53670\n",
      "train data sequences shape:  torch.Size([53670, 600])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Last Char Test:  61%|██████▏   | 129/210 [27:51<17:27, 12.93s/it]"
     ]
    }
   ],
   "source": [
    "accuracy, words_text = test_last_char_text(model, dataset_path='/kaggle/input/project-data/data/')\n",
    "print(accuracy)\n",
    "print(words_text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T03:21:22.093651Z",
     "iopub.status.busy": "2025-12-09T03:21:22.092870Z",
     "iopub.status.idle": "2025-12-09T03:21:22.102570Z",
     "shell.execute_reply": "2025-12-09T03:21:22.101933Z",
     "shell.execute_reply.started": "2025-12-09T03:21:22.093625Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ظاهرةً->ظاهرةٌ\n",
      "وبحقٍّ->وبحقٍّ\n",
      "بينٍ->بينَ\n",
      "تضعفُ->تضعفِ\n",
      "التهمةُ->التهمةِ\n",
      "وهوَ->وهوَ\n",
      "الفرقُ->الفرقُ\n",
      "بينهُ->بينهُ\n",
      "وبينَ->وبينَ\n",
      "الشهادةِ->الشهادةِ\n",
      "وعنْ->وعنْ\n",
      "أصبغَ->أصبغَ\n",
      "الجوازُ->الجوازُ\n",
      "في->في\n",
      "الولدِ->الولدِ\n",
      "والزوجةِ->والزوجةِ\n",
      "والأخِ->والأخِ\n",
      "والمكاتبِ->والمكاتبِ\n",
      "والمدبرِ->والمدبرِ\n",
      "والمديانِ->والمديانِ\n",
      "إنْ->إنْ\n",
      "كانَ->كانَ\n",
      "منْ->منْ\n",
      "أهلِ->أهلِ\n",
      "القيامِ->القيامِ\n",
      "بالحقِّ->بالحقِّ\n",
      "وصحَّ->وصحَّ\n",
      "الحكمُ->الحكمُ\n",
      "وقدْ->وقدْ\n",
      "يحكمُ->يحكمُ\n",
      "للخليفةِ->للخليفةِ\n",
      "وهوَ->وهوَ\n",
      "فوقهُ->فوقهُ\n",
      "وتهمتهُ->وتهمتهُ\n",
      "أقوى->أقوى\n",
      "ولا->ولا\n",
      "ينبغي->ينبغي\n",
      "لهُ->لهُ\n",
      "القضاءُ->القضاءُ\n",
      "بينَ->بينَ\n",
      "أحدٍ->أحدٍ\n",
      "منْ->منْ\n",
      "عشيرتهِ->عشيرتهِ\n",
      "وخصمهِ->وخصمهِ\n",
      "وإنْ->وإنْ\n",
      "رضيَ->رضيَ\n",
      "الخصمُ->الخصمُ\n",
      "بخلافِ->بخلافِ\n",
      "رجلينِ->رجلينِ\n",
      "رضيا->رضيا\n",
      "بحكمِ->بحكمِ\n",
      "رجلٍ->رجلٍ\n",
      "أجنبيٍّ->أجنبيٍّ\n",
      "فينفذُ->فينفذُ\n",
      "ذلكَ->ذلكَ\n",
      "عليهما->عليهما\n",
      "ولا->ولا\n",
      "يقضي->يقضي\n",
      "بينهُ->بينهُ\n"
     ]
    }
   ],
   "source": [
    "# Mapping of Unicode codes to Arabic diacritics\n",
    "unicode_to_diacritic = {\n",
    "    1614: 'َ',   # fath\n",
    "    1611: 'ً',   # tanween bel fath\n",
    "    1615: 'ُ',   # damm\n",
    "    1612: 'ٌ',   # tanween bel damm\n",
    "    1616: 'ِ',   # kasr\n",
    "    1613: 'ٍ',   # tanween bel kasr\n",
    "    1618: 'ْ',   # sukun\n",
    "    1617: 'ّ',   # shadd\n",
    "    0: '',       # no diacritic\n",
    "}\n",
    "\n",
    "# Your input list as strings\n",
    "input_list = [\n",
    "    \"ظاهرة:1611->1612\",\n",
    "    \"وبحق:(1617, 1613)->(1617, 1613)\",\n",
    "    \"بين:1613->1614\",\n",
    "    \"تضعف:1615->1616\",\n",
    "    \"التهمة:1615->1616\",\n",
    "    \"وهو:1614->1614\",\n",
    "    \"الفرق:1615->1615\",\n",
    "    \"بينه:1615->1615\",\n",
    "    \"وبين:1614->1614\",\n",
    "    \"الشهادة:1616->1616\",\n",
    "    \"وعن:1618->1618\",\n",
    "    \"أصبغ:1614->1614\",\n",
    "    \"الجواز:1615->1615\",\n",
    "    \"في:0->0\",\n",
    "    \"الولد:1616->1616\",\n",
    "    \"والزوجة:1616->1616\",\n",
    "    \"والأخ:1616->1616\",\n",
    "    \"والمكاتب:1616->1616\",\n",
    "    \"والمدبر:1616->1616\",\n",
    "    \"والمديان:1616->1616\",\n",
    "    \"إن:1618->1618\",\n",
    "    \"كان:1614->1614\",\n",
    "    \"من:1618->1618\",\n",
    "    \"أهل:1616->1616\",\n",
    "    \"القيام:1616->1616\",\n",
    "    \"بالحق:(1617, 1616)->(1617, 1616)\",\n",
    "    \"وصح:(1617, 1614)->(1617, 1614)\",\n",
    "    \"الحكم:1615->1615\",\n",
    "    \"وقد:1618->1618\",\n",
    "    \"يحكم:1615->1615\",\n",
    "    \"للخليفة:1616->1616\",\n",
    "    \"وهو:1614->1614\",\n",
    "    \"فوقه:1615->1615\",\n",
    "    \"وتهمته:1615->1615\",\n",
    "    \"أقوى:0->0\",\n",
    "    \"ولا:0->0\",\n",
    "    \"ينبغي:0->0\",\n",
    "    \"له:1615->1615\",\n",
    "    \"القضاء:1615->1615\",\n",
    "    \"بين:1614->1614\",\n",
    "    \"أحد:1613->1613\",\n",
    "    \"من:1618->1618\",\n",
    "    \"عشيرته:1616->1616\",\n",
    "    \"وخصمه:1616->1616\",\n",
    "    \"وإن:1618->1618\",\n",
    "    \"رضي:1614->1614\",\n",
    "    \"الخصم:1615->1615\",\n",
    "    \"بخلاف:1616->1616\",\n",
    "    \"رجلين:1616->1616\",\n",
    "    \"رضيا:0->0\",\n",
    "    \"بحكم:1616->1616\",\n",
    "    \"رجل:1613->1613\",\n",
    "    \"أجنبي:(1617, 1613)->(1617, 1613)\",\n",
    "    \"فينفذ:1615->1615\",\n",
    "    \"ذلك:1614->1614\",\n",
    "    \"عليهما:0->0\",\n",
    "    \"ولا:0->0\",\n",
    "    \"يقضي:0->0\",\n",
    "    \"بينه:1615->1615\"\n",
    "]\n",
    "\n",
    "# Process list to apply last-character diacritics\n",
    "output_list = []\n",
    "\n",
    "for item in input_list:\n",
    "    word, label = item.split(\":\")\n",
    "    true_label, pred_label = label.split(\"->\")\n",
    "\n",
    "    def parse_label(lbl):\n",
    "        if lbl.startswith(\"(\"):\n",
    "            parts = lbl.strip(\"()\").split(\",\")\n",
    "            return ''.join([unicode_to_diacritic[int(p)] for p in parts])\n",
    "        else:\n",
    "            return unicode_to_diacritic[int(lbl)]\n",
    "\n",
    "    true_diac = parse_label(true_label)\n",
    "    pred_diac = parse_label(pred_label)\n",
    "\n",
    "    if len(word) > 0:\n",
    "        word_true = word[:-1] + word[-1] + true_diac\n",
    "        word_pred = word[:-1] + word[-1] + pred_diac\n",
    "    else:\n",
    "        word_true = true_diac\n",
    "        word_pred = pred_diac\n",
    "\n",
    "    output_list.append(f\"{word_true}->{word_pred}\")\n",
    "\n",
    "# output_list now contains the words with last-character diacritics\n",
    "for w in output_list:\n",
    "    print(w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-12-09T02:03:50.467185Z",
     "iopub.status.idle": "2025-12-09T02:03:50.467494Z",
     "shell.execute_reply": "2025-12-09T02:03:50.467355Z",
     "shell.execute_reply.started": "2025-12-09T02:03:50.467337Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# from huggingface_hub import HfApi, HfFolder, Repository, upload_file\n",
    "\n",
    "# # login (once)\n",
    "# from huggingface_hub import login\n",
    "\n",
    "# # repository info\n",
    "# repo_id = \"OmarHashem80/CharLSTM\"  # replace with your HF username and model repo name\n",
    "# model_filename = \"/kaggle/input/rnn97/pytorch/default/1/BiLSTM_Loss=0.0325396_Accuracy=97.768%_embedding_size=300hidden_size=256lr=0.001num_layers=5num_epochs=19max_len=600batch_size=32.pkl\"\n",
    "# # meta_filename = \"best_model_meta.json\"\n",
    "\n",
    "# # upload files\n",
    "# upload_file(\n",
    "#     path_or_fileobj=model_filename,\n",
    "#     path_in_repo=model_filename,\n",
    "#     repo_id=repo_id,\n",
    "#     repo_type=\"model\"\n",
    "# )\n",
    "\n",
    "# # upload_file(\n",
    "# #     # path_or_fileobj=meta_filename,\n",
    "# #     # path_in_repo=meta_filename,\n",
    "# #     repo_id=repo_id,\n",
    "# #     repo_type=\"model\"\n",
    "# # )\n",
    "\n",
    "# print(f\"Model and metadata uploaded to Hugging Face Hub at {repo_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T10:36:15.095137Z",
     "iopub.status.busy": "2025-12-09T10:36:15.094599Z",
     "iopub.status.idle": "2025-12-09T10:36:20.230018Z",
     "shell.execute_reply": "2025-12-09T10:36:20.228762Z",
     "shell.execute_reply.started": "2025-12-09T10:36:15.095114Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96825d1ee4784315ab6de28cf55f53e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "best_model.pkl:   0%|          | 0.00/1.19G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e76a840ddaf43aaa808a7f88db72093",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "best_model_meta.json:   0%|          | 0.00/339 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded model: /root/.cache/huggingface/hub/models--OmarHashem80--LSTM98/snapshots/064a2a433e8f9d0d2a72b0f2185af280380d6fe8/best_model.pkl\n",
      "Model saved to /kaggle/working/best_model.pkl\n",
      "Model saved to /kaggle/working/best_model_meta.json\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "from huggingface_hub import login\n",
    "login(\"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\")\n",
    "repo_id = \"OmarHashem80/LSTM98\"\n",
    "model_file = \"best_model.pkl\"\n",
    "meta_file = \"best_model_meta.json\"\n",
    "\n",
    "# download from HF Hub\n",
    "local_model_path = hf_hub_download(repo_id=repo_id, filename=model_file)\n",
    "local_meta_path = hf_hub_download(repo_id=repo_id, filename=meta_file)\n",
    "\n",
    "print(\"Downloaded model:\", local_model_path)\n",
    "import shutil\n",
    "\n",
    "destination_path = '/kaggle/working/best_model.pkl'\n",
    "\n",
    "shutil.copy(local_model_path, destination_path)\n",
    "print(f'Model saved to {destination_path}')\n",
    "\n",
    "destination_path = '/kaggle/working/best_model_meta.json'\n",
    "\n",
    "shutil.copy(local_meta_path, destination_path)\n",
    "print(f'Model saved to {destination_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-12-09T02:03:50.469897Z",
     "iopub.status.idle": "2025-12-09T02:03:50.470209Z",
     "shell.execute_reply": "2025-12-09T02:03:50.470062Z",
     "shell.execute_reply.started": "2025-12-09T02:03:50.470048Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import plotly.express as px\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# import pandas as pd\n",
    "\n",
    "# # Move tensors to CPU\n",
    "# y_true = out[0].detach().cpu()\n",
    "# y_pred = out[1].detach().cpu()\n",
    "\n",
    "# # Convert logits / one-hot → class indices\n",
    "# if y_true.ndim > 1:\n",
    "#     y_true = y_true.argmax(dim=1)\n",
    "# if y_pred.ndim > 1:\n",
    "#     y_pred = y_pred.argmax(dim=1)\n",
    "\n",
    "# # Convert to numpy\n",
    "# y_true = y_true.numpy()\n",
    "# y_pred = y_pred.numpy()\n",
    "\n",
    "# # Confusion matrix\n",
    "# cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# # Make dataframe for nicer Plotly display\n",
    "# labels = list(range(cm.shape[0]))\n",
    "# df = pd.DataFrame(cm, index=labels, columns=labels)\n",
    "\n",
    "# fig = px.imshow(\n",
    "#     df,\n",
    "#     text_auto=True,\n",
    "#     color_continuous_scale=\"Blues\",\n",
    "#     labels=dict(x=\"Predicted\", y=\"Actual\", color=\"Count\"),\n",
    "#     title=\"Confusion Matrix Heatmap (Plotly)\"\n",
    "# )\n",
    "\n",
    "# fig.update_layout(width=600, height=500)\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8846952,
     "sourceId": 13885877,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8915573,
     "sourceId": 13987393,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8903667,
     "sourceId": 14071327,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8957446,
     "sourceId": 14072039,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
